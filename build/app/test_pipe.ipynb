{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION = 'us-west2'\n",
    "\n",
    "import os\n",
    "os.environ['key'] = key\n",
    "os.environ['secret'] = secret\n",
    "os.environ['REGION'] = REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "AWS_SERVER_PUBLIC_KEY = os.environ['key']\n",
    "\n",
    "AWS_SERVER_SECRET_KEY = os.environ['secret']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dill as pickle\n",
    "import os\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import pytesseract\n",
    "from PIL import Image, ImageEnhance, ImageFilter\n",
    "from collections import defaultdict\n",
    "import shutil\n",
    "\n",
    "\n",
    "# TODO remove this \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "\n",
    "# from .utils import unpickle\n",
    "# TODO Move to utils \n",
    "def unpickle(filename):\n",
    "    \"\"\" Unpickle file \"\"\"\n",
    "    with open(filename, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "\n",
    "# DATA_DIR = '../../../data'\n",
    "\n",
    "DATA_DIR = '../../data'\n",
    "# TODO move this \n",
    "data_path = os.path.join(DATA_DIR, 'raw/word_level')\n",
    "meta_json_data_path = os.path.join(DATA_DIR, 'preprocessed/meta.json')\n",
    "word_level_meta_path = os.path.join(DATA_DIR, 'preprocessed/word_level_meta.csv')\n",
    "word_path_mapping_path = os.path.join(DATA_DIR, 'processed/word_path_mapping.pkl')\n",
    "X_path = os.path.join(DATA_DIR, 'processed/X.npy')\n",
    "y_path = os.path.join(DATA_DIR, 'processed/y.npy')\n",
    "bigram_model_path = os.path.join(DATA_DIR, 'processed/ngram_models/bigram_likelihood_model.pkl')\n",
    "trigram_model_path = os.path.join(DATA_DIR, 'processed/ngram_models/trigram_likelihood_model.pkl')\n",
    "letters_path = os.path.join(DATA_DIR, 'processed/letters_map.pkl')\n",
    "\n",
    "\n",
    "import boto3\n",
    "from io import BytesIO\n",
    "import dill as pickle\n",
    "\n",
    "\n",
    "# client = boto3.resource('s3')\n",
    "# bucket = client.Bucket('handwrittingdetection')\n",
    "\n",
    "# session = boto3.Session(\n",
    "#     aws_access_key_id=settings.AWS_SERVER_PUBLIC_KEY,\n",
    "#     aws_secret_access_key=settings.AWS_SERVER_SECRET_KEY,\n",
    "# )\n",
    "# client = session.resource('s3')\n",
    "\n",
    "# s3_client = boto3.client('s3', \n",
    "#                       aws_access_key_id=AWS_SERVER_PUBLIC_KEY, \n",
    "#                       aws_secret_access_key=AWS_SERVER_SECRET_KEY, \n",
    "#                       region_name='us-west2'\n",
    "#                       )\n",
    "\n",
    "\n",
    "def s3_init(bucketname='handwrittingdetection'):\n",
    "\n",
    "    \n",
    "    session = boto3.Session(\n",
    "        aws_access_key_id=AWS_SERVER_PUBLIC_KEY,\n",
    "        aws_secret_access_key=AWS_SERVER_SECRET_KEY,\n",
    "    )\n",
    "    \n",
    "    client = session.resource('s3')\n",
    "    bucket = client.Bucket(bucketname)\n",
    "    return client, bucket\n",
    "\n",
    "# client, bucket = s3_init(bucketname='handwrittingdetection')\n",
    "    \n",
    "def unpickle_s3(filename, client=None, bucket=None):\n",
    "    with BytesIO() as data:\n",
    "        bucket.download_fileobj(filename, data)\n",
    "        data.seek(0)\n",
    "        return pickle.load(data)\n",
    "\n",
    "\n",
    "def tokenize_and_join(context):\n",
    "    tokens = word_tokenize(context)\n",
    "    context = ' '.join(tokens)\n",
    "    return tokens, context\n",
    "\n",
    "def ngram_backoff_model(left_text, right_text, trigram_model, bigram_model, OOV_token=0):\n",
    "\n",
    "\n",
    "    left_tokens, left_text = tokenize_and_join(left_text)\n",
    "    right_tokens, right_text = tokenize_and_join(right_text)\n",
    "    full_text = left_text + ' [] '  + right_text\n",
    "\n",
    "    # get previous word(s)\n",
    "    try:\n",
    "        prev_word = left_tokens[-1]\n",
    "    except:\n",
    "        prev_word = '<bos>'\n",
    "    try:\n",
    "        prev_prev_word = left_tokens[-2]\n",
    "    except:\n",
    "        prev_prev_word = '<bos>'\n",
    "    # model preds \n",
    "#     print(prev_prev_word, prev_word)\n",
    "    pred = trigram_model[(prev_prev_word, prev_word)]\n",
    "#     print('pred1', pred)\n",
    "    if pred == OOV_token:\n",
    "        pred = bigram_model[(prev_word)]\n",
    "#         print('pred2', pred)\n",
    "        if pred == OOV_token:\n",
    "            pred = 'UNK'\n",
    "    return pred \n",
    "\n",
    "\n",
    "def teseract_baseline(file_url, word_path_mapping, letters, tmpdir='tmp/'):\n",
    "    img_width = 256\n",
    "    img_height = 100\n",
    "    # gets image from sample index\n",
    "    # img = word_path_mapping[y[sample_index][0]]\n",
    "    im = Image.open(file_url)  # img is the path of the image\n",
    "    im = im.convert(\"RGBA\")\n",
    "    im = im.resize((img_width, img_height))\n",
    "    newimdata = []\n",
    "    datas = im.getdata()\n",
    "\n",
    "    vals = 255\n",
    "\n",
    "    for item in datas:\n",
    "        if item[0] < vals or item[1] < vals or item[2] < vals:\n",
    "            newimdata.append(item)\n",
    "        else:\n",
    "            newimdata.append((255, 255, 255))\n",
    "    im.putdata(newimdata)\n",
    "\n",
    "    im = im.filter(ImageFilter.MedianFilter())\n",
    "    enhancer = ImageEnhance.Contrast(im)\n",
    "    im = enhancer.enhance(2)\n",
    "    im = im.convert('1')\n",
    "    \n",
    "    if not os.path.exists(tmpdir):\n",
    "        os.makedirs(tmpdir)\n",
    "    save_img_path = os.path.join(tmpdir, 'temp_img.jpg')\n",
    "    im.save(save_img_path)\n",
    "    \n",
    "    text = pytesseract.image_to_string(Image.open(save_img_path),\n",
    "                config='-c tessedit_char_whitelist=0123456789abcdefghijklmnopqrstuvwxyz -psm 13', lang='eng')\n",
    "    \n",
    "    # os.remove(save_img_path)\n",
    "    # os.rmdir(tmpdir)\n",
    "    # shutil.rmtree(tmpdir)\n",
    "    return text, len(text) \n",
    "\n",
    "\n",
    "    \n",
    "def get_ocr_model_pred(file_url, word_path_mapping, letters):\n",
    "    ocr_pred, len_pred = teseract_baseline(file_url, word_path_mapping, letters)\n",
    "    return ocr_pred, len_pred\n",
    "\n",
    "def get_language_model_pred(left_text, right_text, trigram_model, bigram_model):\n",
    "    pred = ngram_backoff_model(left_text, right_text, trigram_model, bigram_model)\n",
    "    return pred\n",
    "\n",
    "\n",
    "def get_pos_tags(left_text, right_text):\n",
    "    return []\n",
    "\n",
    "def weight_features(left_text, right_text, file_url, trigram_model, bigram_model, word_path_mapping, letters, weights={}):\n",
    "    ocr_pred, len_pred = get_ocr_model_pred(file_url, word_path_mapping, letters)\n",
    "    print('OCR', ocr_pred, len_pred)\n",
    "    lm_pred = get_language_model_pred(left_text, right_text, trigram_model, bigram_model)\n",
    "    print('LM', lm_pred)\n",
    "    pos_pred = get_pos_tags(left_text, right_text)\n",
    "    \n",
    "    return ocr_pred, len_pred, lm_pred\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def absoluteFilePaths(directory):\n",
    "    \"\"\"Walk filepaths\"\"\"\n",
    "    for dirpath,_,filenames in os.walk(directory):\n",
    "        for f in filenames:\n",
    "            yield os.path.join(dirpath, f)\n",
    "\n",
    "\n",
    "def create_image_path(df, data_path, use_s3=False, s3_image_path='data/word_level'):\n",
    "    \"\"\"Create dictionary for mapping of word to data path\"\"\"\n",
    "    if use_s3:\n",
    "        files = list(bucket.objects.filter(Prefix='data/word_level/sample'))\n",
    "        all_paths = [f.key for f in files if '.png' in f.key]\n",
    "    else:\n",
    "        all_paths = [i for i in absoluteFilePaths(data_path)]\n",
    "        \n",
    "    all_path_dict = defaultdict(lambda: 0, dict(zip(all_path_endings, all_paths)))\n",
    "    all_path_endings = [i.split('/')[-1].split('.')[0] for i in all_paths]\n",
    "    defaultdict(lambda: 0, dict(zip(all_path_endings, all_paths)))\n",
    "    df['image_path'] = df['image_name'].map(lambda x: all_path_dict[x])\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def get_prediction(left_text, right_text, file_url, use_s3=False):\n",
    "\n",
    "    if use_s3:\n",
    "\n",
    "        client, bucket = s3_init(bucketname='handwrittingdetection')\n",
    "\n",
    "        bigram_model_path = 'data/ngram_models/bigram_likelihood_model.pkl'\n",
    "        trigram_model_path = 'data/ngram_models/trigram_likelihood_model.pkl'\n",
    "\n",
    "        word_path_mapping_path = 'data/word_path_mapping.pkl'\n",
    "        letters_path = 'data/letters_map.pkl'\n",
    "\n",
    "        bigram_model = unpickle_s3(bigram_model_path, client, bucket)\n",
    "        trigram_model = unpickle_s3(trigram_model_path, client, bucket)\n",
    "\n",
    "        word_path_mapping = unpickle_s3(word_path_mapping_path, client, bucket)\n",
    "        letter2idx = unpickle_s3(letters_path, client, bucket)\n",
    "\n",
    "\n",
    "    else:\n",
    "        data_path = os.path.join(DATA_DIR, 'raw/word_level')\n",
    "        meta_json_data_path = os.path.join(DATA_DIR, 'preprocessed/meta.json')\n",
    "        word_level_meta_path = os.path.join(DATA_DIR, 'preprocessed/word_level_meta.csv')\n",
    "        word_path_mapping_path = os.path.join(DATA_DIR, 'processed/word_path_mapping.pkl')\n",
    "        bigram_model_path = os.path.join(DATA_DIR, 'processed/ngram_models/bigram_likelihood_model.pkl')\n",
    "        trigram_model_path = os.path.join(DATA_DIR, 'processed/ngram_models/trigram_likelihood_model.pkl')\n",
    "        letters_path = os.path.join(DATA_DIR, 'processed/letters_map.pkl')\n",
    "\n",
    "        # Language Models\n",
    "        bigram_model = unpickle(bigram_model_path)\n",
    "        trigram_model = unpickle(trigram_model_path)\n",
    "\n",
    "        # Meta\n",
    "        meta = pd.read_json(meta_json_data_path)\n",
    "        word_level_df = pd.read_csv(word_level_meta_path)\n",
    "        word_level_df = create_image_path(word_level_df, data_path)\n",
    "\n",
    "        # word_path_mapping = defaultdict(lambda: 0, dict(zip(word_level_df.token, word_level_df.image_path)))                              \n",
    "        # with open(os.path.join(DATA_DIR, 'processed', 'word_path_mapping.pkl'), 'wb') as f:\n",
    "        #     pickle.dump(word_path_mapping, f)\n",
    "\n",
    "        word_path_mapping = unpickle(word_path_mapping_path)\n",
    "        letter2idx = unpickle(letters_path)\n",
    "\n",
    "    letters = list(letter2idx.keys())\n",
    "    letters = ''.join(letters[1:-2])\n",
    "\n",
    "    left_text = left_text.rstrip()\n",
    "    right_text = right_text.rstrip()\n",
    "\n",
    "    ocr_pred, len_pred, lm_pred = weight_features(left_text, right_text, file_url, trigram_model, bigram_model, word_path_mapping, letters)\n",
    "    return ocr_pred, len_pred, lm_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OCR wk 2\n",
      "dog ran\n",
      "pred1 0\n",
      "pred2 a\n",
      "LM a\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('wk', 2, 'a')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "left_text = 'the dog ran'\n",
    "right_text = 'the house'\n",
    "file_path = '../../data/raw/word_level/a01/a01-007u/a01-007u-00-05.png'\n",
    "\n",
    "get_prediction(left_text, right_text, file_path, use_s3=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=L size=152x72 at 0x1115C9048>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image.open(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "client, bucket = s3_init(bucketname='handwrittingdetection')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file = list(bucket.objects.filter(Prefix='models/language_model/model.param'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[s3.ObjectSummary(bucket_name='handwrittingdetection', key='models/language_model/model.param'),\n",
       " s3.ObjectSummary(bucket_name='handwrittingdetection', key='models/language_model/model.param.config.json'),\n",
       " s3.ObjectSummary(bucket_name='handwrittingdetection', key='models/language_model/model.param.optim')]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Context2vec(\n",
       "  (drop): Dropout(p=0.0)\n",
       "  (l2r_emb): Embedding(32775, 300, padding_idx=1)\n",
       "  (l2r_rnn): LSTM(300, 600, batch_first=True)\n",
       "  (r2l_emb): Embedding(32775, 300, padding_idx=1)\n",
       "  (r2l_rnn): LSTM(300, 600, batch_first=True)\n",
       "  (criterion): NegativeSampling(\n",
       "    (W): Embedding(32775, 600, padding_idx=1)\n",
       "    (logsigmoid): LogSigmoid()\n",
       "  )\n",
       "  (MLP): MLP(\n",
       "    (drop): Dropout(p=0.0)\n",
       "    (MLP): ModuleList(\n",
       "      (0): Linear(in_features=1200, out_features=1200, bias=True)\n",
       "      (1): Linear(in_features=1200, out_features=600, bias=True)\n",
       "    )\n",
       "    (activation_function): ReLU()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "from torch import optim\n",
    "from models.context2vec.src.eval.mscc import mscc_evaluation\n",
    "from models.context2vec.src.core.nets import Context2vec\n",
    "from models.context2vec.src.util.args import parse_args\n",
    "from models.context2vec.src.util.batch import Dataset\n",
    "from models.context2vec.src.util.config import Config\n",
    "from models.context2vec.src.util.io import write_embedding, write_config, read_config, load_vocab\n",
    "device = 'cpu'\n",
    "# args = parse_args()\n",
    "modelfile = 'models/context2vec/models/model.param'\n",
    "# modelfile = model_file[0].key\n",
    "wordsfile = 'models/context2vec/models/embedding.vec'\n",
    "config_file = modelfile+'.config.json'\n",
    "config_dict = read_config(config_file)\n",
    "model = Context2vec(vocab_size=config_dict['vocab_size'],\n",
    "                    counter=[1]*config_dict['vocab_size'],\n",
    "                    word_embed_size=config_dict['word_embed_size'],\n",
    "                    hidden_size=config_dict['hidden_size'],\n",
    "                    n_layers=config_dict['n_layers'],\n",
    "                    bidirectional=config_dict['bidirectional'],\n",
    "                    use_mlp=config_dict['use_mlp'],\n",
    "                    dropout=config_dict['dropout'],\n",
    "                    pad_index=config_dict['pad_index'],\n",
    "                    device=device,\n",
    "                    inference=True).to(device)\n",
    "model.load_state_dict(torch.load(modelfile, map_location='cpu'))\n",
    "optimizer = optim.Adam(model.parameters(), lr=config_dict['learning_rate'])\n",
    "# optimizer.load_state_dict(torch.load(modelfile+'.optim'))\n",
    "itos, stoi = load_vocab(wordsfile)\n",
    "unk_token = config_dict['unk_token']\n",
    "bos_token = config_dict['bos_token']\n",
    "eos_token = config_dict['eos_token']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "from torch import optim\n",
    "from models.context2vec.src.eval.mscc import mscc_evaluation\n",
    "from models.context2vec.src.core.nets import Context2vec\n",
    "from models.context2vec.src.util.args import parse_args\n",
    "from models.context2vec.src.util.batch import Dataset\n",
    "from models.context2vec.src.util.config import Config\n",
    "from models.context2vec.src.util.io import write_embedding, write_config, read_config, load_vocab\n",
    "\n",
    "class Inference():\n",
    "    def __init__(self, modelfile, wordsfile, config_file, device='cpu'):\n",
    "        \n",
    "        # args = parse_args()\n",
    "        modelfile = 'models/context2vec/models/model.param'\n",
    "        # modelfile = model_file[0].key\n",
    "        wordsfile = 'models/context2vec/models/embedding.vec'\n",
    "        config_file = modelfile+'.config.json'\n",
    "        config_dict = read_config(config_file)\n",
    "        model = Context2vec(vocab_size=config_dict['vocab_size'],\n",
    "                            counter=[1]*config_dict['vocab_size'],\n",
    "                            word_embed_size=config_dict['word_embed_size'],\n",
    "                            hidden_size=config_dict['hidden_size'],\n",
    "                            n_layers=config_dict['n_layers'],\n",
    "                            bidirectional=config_dict['bidirectional'],\n",
    "                            use_mlp=config_dict['use_mlp'],\n",
    "                            dropout=config_dict['dropout'],\n",
    "                            pad_index=config_dict['pad_index'],\n",
    "                            device=device,\n",
    "                            inference=True).to(device)\n",
    "        model.load_state_dict(torch.load(modelfile, map_location='cpu'))\n",
    "#         optimizer = optim.Adam(model.parameters(), lr=config_dict['learning_rate'])\n",
    "        # optimizer.load_state_dict(torch.load(modelfile+'.optim'))\n",
    "        itos, stoi = load_vocab(wordsfile)\n",
    "        unk_token = config_dict['unk_token']\n",
    "        bos_token = config_dict['bos_token']\n",
    "        eos_token = config_dict['eos_token']\n",
    "        \n",
    "        \n",
    "    def _return_split_sentence(self, sentence):\n",
    "        if ' ' not in sentence:\n",
    "            print('sentence should contain white space to split it into tokens')\n",
    "            raise SyntaxError\n",
    "        elif '[]' not in sentence:\n",
    "            print('sentence should contain `[]` that notes the target')\n",
    "            raise SyntaxError\n",
    "        else:\n",
    "            tokens = sentence.lower().strip().split()\n",
    "            target_pos = tokens.index('[]')\n",
    "            return tokens, target_pos\n",
    "        \n",
    "    def run_inference_by_user_input(self, sentence, model, itos, stoi, unk_token, bos_token, eos_token, device, topK=30):\n",
    "\n",
    "        # evaluation mode \n",
    "        model.eval()\n",
    "        # norm_weight\n",
    "        model.norm_embedding_weight(model.criterion.W)\n",
    "\n",
    "        tokens, target_pos = return_split_sentence(sentence)\n",
    "        tokens[target_pos] = unk_token\n",
    "        tokens = [bos_token] + tokens + [eos_token]\n",
    "        indexed_sentence = [stoi[token] if token in stoi else stoi[unk_token] for token in tokens]\n",
    "        input_tokens = \\\n",
    "            torch.tensor(indexed_sentence, dtype=torch.long, device=device).unsqueeze(0)\n",
    "        topv, topi = model.run_inference(input_tokens, target=None, target_pos=target_pos, k=topK)\n",
    "        output = []  \n",
    "        for value, key in zip(topv, topi):\n",
    "            output.append((value.item(), itos[key.item()]))\n",
    "            print(value.item(), itos[key.item()])\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference_by_user_input(sentence, \n",
    "                                model,\n",
    "                                itos,\n",
    "                                stoi,\n",
    "                                unk_token,\n",
    "                                bos_token,\n",
    "                                eos_token,\n",
    "                                device):\n",
    "\n",
    "    def return_split_sentence(sentence):\n",
    "        if ' ' not in sentence:\n",
    "            print('sentence should contain white space to split it into tokens')\n",
    "            raise SyntaxError\n",
    "        elif '[]' not in sentence:\n",
    "            print('sentence should contain `[]` that notes the target')\n",
    "            raise SyntaxError\n",
    "        else:\n",
    "            tokens = sentence.lower().strip().split()\n",
    "            target_pos = tokens.index('[]')\n",
    "            return tokens, target_pos\n",
    "\n",
    "    ''' \n",
    "    norm_weight\n",
    "    '''\n",
    "    # evaluation mode \n",
    "    model.eval()\n",
    "    \n",
    "    model.norm_embedding_weight(model.criterion.W)\n",
    "\n",
    "    tokens, target_pos = return_split_sentence(sentence)\n",
    "    tokens[target_pos] = unk_token\n",
    "    tokens = [bos_token] + tokens + [eos_token]\n",
    "    indexed_sentence = [stoi[token] if token in stoi else stoi[unk_token] for token in tokens]\n",
    "    input_tokens = torch.tensor(indexed_sentence, dtype=torch.long, device=device).unsqueeze(0)\n",
    "    topv, topi = model.run_inference(input_tokens, target=None, target_pos=target_pos)\n",
    "    output = []  \n",
    "    for value, key in zip(topv, topi):\n",
    "        output.append((value.item(), itos[key.item()]))\n",
    "        print(value.item(), itos[key.item()])\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelfile = 'models/context2vec/models/model.param'\n",
    "wordsfile = 'models/context2vec/models/embedding.vec'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.8051932454109192, 'into'), (0.56807941198349, 'above'), (0.256673127412796, 'through'), (0.0, '<PAD>'), (0.0, '<EOS>'), (0.0, '<BOS>'), (-0.0033718030899763107, 'against'), (-0.024189310148358345, 'across'), (-0.09841619431972504, 'alongside'), (-0.25158366560935974, 'from')]\n"
     ]
    }
   ],
   "source": [
    "left_text = 'the dog ran'\n",
    "right_text = 'the house'\n",
    "\n",
    "sentence = left_text + ' [] ' + right_text\n",
    "\n",
    "run_inference_by_user_input(sentence, model, itos, stoi, unk_token=unk_token, \n",
    "                            bos_token=bos_token, eos_token=eos_token, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.param             model.param.config.json\r\n"
     ]
    }
   ],
   "source": [
    "!ls models/context2vec/models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__init__.py      run.py           \u001b[1m\u001b[34mtemplates\u001b[m\u001b[m/       \u001b[1m\u001b[34mtmp\u001b[m\u001b[m/\r\n",
      "\u001b[1m\u001b[34mmodels\u001b[m\u001b[m/          \u001b[1m\u001b[34mstatic\u001b[m\u001b[m/          test_pipe.ipynb  utils.py\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
