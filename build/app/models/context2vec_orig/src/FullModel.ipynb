{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchtext.data import Field, Dataset, Example, Iterator\n",
    "import numpy as np\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset.py\r\n"
     ]
    }
   ],
   "source": [
    "ls ../../context2vec2/src/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile ../../context2vec2/src/dataset.py\n",
    "\n",
    "class WikiDataset:\n",
    "    def __init__(self, X, batch_size, min_freq, device, pad_token='<PAD>', unk_token='<UNK>', \n",
    "                                      bos_token='<BOS>', eos_token='<EOS>', seed=100):\n",
    "        super().__init__() \n",
    "        np.random.seed(seed)\n",
    "        self.sent_dict = self._gathered_by_lengths(X)\n",
    "        self.pad_token = pad_token\n",
    "        self.unk_token = unk_token\n",
    "        self.bos_token = bos_token\n",
    "        self.eos_token = eos_token\n",
    "        self.device = device\n",
    "        # set up torchtext Fields\n",
    "        self.sentence_field = Field(use_vocab=True, unk_token=self.unk_token, pad_token=self.pad_token,\n",
    "                                         init_token=self.bos_token, eos_token=self.eos_token,\n",
    "                                         batch_first=True, include_lengths=False)\n",
    "        self.sentence_field_id = Field(use_vocab=False, batch_first=True)\n",
    "        # build vocal\n",
    "        self.sentence_field.build_vocab(X, min_freq=min_freq)\n",
    "        self.vocab = self.sentence_field.vocab\n",
    "        if self.pad_token: self.pad_idx = self.sentence_field.vocab.stoi[self.pad_token]\n",
    "        self.dataset = self._create_dataset(self.sent_dict, X)\n",
    "    \n",
    "    def get_raw_sentence(self, X):\n",
    "        return [[self.vocab.itos[idx] for idx in sentence] for sentence in X]   \n",
    "     \n",
    "        \n",
    "    def _gathered_by_lengths(self, X):\n",
    "        lengths = [(index, len(sent)) for index, sent in enumerate(X)]\n",
    "        lengths = sorted(lengths, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        sent_dict = {}\n",
    "        current_length = -1\n",
    "        for i, length in lengths:\n",
    "            if current_length == length:\n",
    "                sent_dict[length].append(i)\n",
    "            else:\n",
    "                sent_dict[length] = [i]\n",
    "                current_length = length\n",
    "\n",
    "        return sent_dict\n",
    "    \n",
    "    def _create_dataset(self, sent_dict, X):\n",
    "        datasets = {}\n",
    "        _fields = [('sentence', self.sentence_field),\n",
    "                   ('id', self.sentence_field_id)]\n",
    "        for length, index in sent_dict.items():\n",
    "            index = np.array(index)\n",
    "            items = [*zip(X[index], index[:, numpy.newaxis])]\n",
    "            datasets[length] = Dataset(self._get_examples(items, _fields), _fields)\n",
    "        return np.random.permutation(list(datasets.values()))\n",
    "    \n",
    "    \n",
    "    def _get_examples(self, items, fields):\n",
    "        return [Example.fromlist(item, fields) for item in items]\n",
    "\n",
    "    \n",
    "    def get_batch_iter(self, batch_size):\n",
    "\n",
    "        def sort(data):\n",
    "            return len(getattr(data, 'sentence'))\n",
    "\n",
    "        for dataset in self.dataset:\n",
    "            yield Iterator(dataset=dataset,\n",
    "                                batch_size=batch_size,\n",
    "                                sort_key=sort,\n",
    "                                train=True,\n",
    "                                repeat=False,\n",
    "                                device=self.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ../../context2vec2/src/walker_alias.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../../context2vec2/src/walker_alias.py\n",
    "\n",
    "import numpy\n",
    "# Taken from here \n",
    "# https://github.com/chainer/chainer/blob/v5.2.0/chainer/utils/walker_alias.py#L6\n",
    "\n",
    "class WalkerAlias(object):\n",
    "    \"\"\"Implementation of Walker's alias method.\n",
    "    This method generates a random sample from given probabilities\n",
    "    :math:`p_1, \\\\dots, p_n` in :math:`O(1)` time.\n",
    "    It is more efficient than :func:`~numpy.random.choice`.\n",
    "    This class works on both CPU and GPU.\n",
    "    Args:\n",
    "        probs (float list): Probabilities of entries. They are normalized with\n",
    "                            `sum(probs)`.\n",
    "    See: `Wikipedia article <https://en.wikipedia.org/wiki/Alias_method>`_\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, probs):\n",
    "        prob = numpy.array(probs, numpy.float32)\n",
    "        prob /= numpy.sum(prob)\n",
    "        threshold = numpy.ndarray(len(probs), numpy.float32)\n",
    "        values = numpy.ndarray(len(probs) * 2, numpy.int32)\n",
    "        il, ir = 0, 0\n",
    "        pairs = list(zip(prob, range(len(probs))))\n",
    "        pairs.sort()\n",
    "        for prob, i in pairs:\n",
    "            p = prob * len(probs)\n",
    "            while p > 1 and ir < il:\n",
    "                values[ir * 2 + 1] = i\n",
    "                p -= 1.0 - threshold[ir]\n",
    "                ir += 1\n",
    "            threshold[il] = p\n",
    "            values[il * 2] = i\n",
    "            il += 1\n",
    "        # fill the rest\n",
    "        for i in range(ir, len(probs)):\n",
    "            values[i * 2 + 1] = 0\n",
    "\n",
    "        assert((values < len(threshold)).all())\n",
    "        self.threshold = threshold\n",
    "        self.values = values\n",
    "        self.use_gpu = False\n",
    "\n",
    "    def to_gpu(self):\n",
    "        \"\"\"Make a sampler GPU mode.\n",
    "        \"\"\"\n",
    "        if not self.use_gpu:\n",
    "            self.threshold = cuda.to_gpu(self.threshold)\n",
    "            self.values = cuda.to_gpu(self.values)\n",
    "            self.use_gpu = True\n",
    "\n",
    "    def to_cpu(self):\n",
    "        \"\"\"Make a sampler CPU mode.\n",
    "        \"\"\"\n",
    "        if self.use_gpu:\n",
    "            self.threshold = cuda.to_cpu(self.threshold)\n",
    "            self.values = cuda.to_cpu(self.values)\n",
    "            self.use_gpu = False\n",
    "\n",
    "    def sample(self, shape):\n",
    "        \"\"\"Generates a random sample based on given probabilities.\n",
    "        Args:\n",
    "            shape (tuple of int): Shape of a return value.\n",
    "        Returns:\n",
    "            Returns a generated array with the given shape. If a sampler is in\n",
    "            CPU mode the return value is a :class:`numpy.ndarray` object, and\n",
    "            if it is in GPU mode the return value is a :class:`cupy.ndarray`\n",
    "            object.\n",
    "        \"\"\"\n",
    "        if self.use_gpu:\n",
    "            return self.sample_gpu(shape)\n",
    "        else:\n",
    "            return self.sample_cpu(shape)\n",
    "\n",
    "    def sample_cpu(self, shape):\n",
    "        ps = numpy.random.uniform(0, 1, shape)\n",
    "        pb = ps * len(self.threshold)\n",
    "        index = pb.astype(numpy.int32)\n",
    "        left_right = (self.threshold[index] < pb - index).astype(numpy.int32)\n",
    "        return self.values[index * 2 + left_right]\n",
    "\n",
    "    def sample_gpu(self, shape):\n",
    "        ps = cuda.cupy.random.uniform(size=shape, dtype=numpy.float32)\n",
    "        vs = cuda.elementwise(\n",
    "            'T ps, raw T threshold , raw S values, int32 b',\n",
    "            'int32 vs',\n",
    "            '''\n",
    "            T pb = ps * b;\n",
    "            int index = __float2int_rd(pb);\n",
    "            // fill_uniform sometimes returns 1.0, so we need to check index\n",
    "            if (index >= b) {\n",
    "              index = 0;\n",
    "            }\n",
    "            int lr = threshold[index] < pb - index;\n",
    "            vs = values[index * 2 + lr];\n",
    "            ''',\n",
    "            'walker_alias_sample'\n",
    "        )(ps, self.threshold, self.values, len(self.threshold))\n",
    "        return vs\n",
    "    \n",
    "  \n",
    "# import numpy\n",
    "    \n",
    "# class WalkerAlias(object):\n",
    "#     '''\n",
    "#     This is from Chainer's implementation.\n",
    "#     You can find the original code at\n",
    "#     https://github.com/chainer/chainer/blob/v4.4.0/chainer/utils/walker_alias.py\n",
    "#     This class is\n",
    "#         Copyright (c) 2015 Preferred Infrastructure, Inc.\n",
    "#         Copyright (c) 2015 Preferred Networks, Inc.\n",
    "#     '''\n",
    "#     def __init__(self, probs):\n",
    "#         prob = numpy.array(probs, numpy.float32)\n",
    "#         prob /= numpy.sum(prob)\n",
    "#         threshold = numpy.ndarray(len(probs), numpy.float32)\n",
    "#         values = numpy.ndarray(len(probs) * 2, numpy.int32)\n",
    "#         il, ir = 0, 0\n",
    "#         pairs = list(zip(prob, range(len(probs))))\n",
    "#         pairs.sort()\n",
    "#         for prob, i in pairs:\n",
    "#             p = prob * len(probs)\n",
    "#             while p > 1 and ir < il:\n",
    "#                 values[ir * 2 + 1] = i\n",
    "#                 p -= 1.0 - threshold[ir]\n",
    "#                 ir += 1\n",
    "#             threshold[il] = p\n",
    "#             values[il * 2] = i\n",
    "#             il += 1\n",
    "#         # fill the rest\n",
    "#         for i in range(ir, len(probs)):\n",
    "#             values[i * 2 + 1] = 0\n",
    "\n",
    "#         assert((values < len(threshold)).all())\n",
    "#         self.threshold = threshold\n",
    "#         self.values = values\n",
    "\n",
    "#     def sample(self, shape):\n",
    "#         ps = numpy.random.uniform(0, 1, shape)\n",
    "#         pb = ps * len(self.threshold)\n",
    "#         index = pb.astype(numpy.int32)\n",
    "#         left_right = (self.threshold[index] < pb - index).astype(numpy.int32)\n",
    "#         return self.values[index * 2 + left_right]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ../../context2vec2/src/negative_sampling.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../../context2vec2/src/negative_sampling.py\n",
    "\n",
    "from walker_alias import WalkerAlias\n",
    "from torch import tensor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "def init_embeddings(x):\n",
    "    x = x.weight.data\n",
    "    value = 2 / (x.size(1) + 1)\n",
    "    x.uniform_(-value, value)\n",
    "    \n",
    "\n",
    "class NegativeSampling(nn.Module):\n",
    "    def __init__(self, embed_size, counter, num_neg, power, device, pad_idx):\n",
    "        super().__init__()\n",
    "        self.counter = counter\n",
    "        self.num_neg = num_neg\n",
    "        self.power = power\n",
    "        self.device = device\n",
    "        \n",
    "        self.W = nn.Embedding(len(counter), embedding_dim=embed_size, padding_idx=pad_idx)\n",
    "        init_embeddings(self.W)\n",
    "        # self.W.weight.data.zero_()\n",
    "        self.log_loss = nn.LogSigmoid()\n",
    "#         self.sum_log_sampled = t.bmm(noise, input.unsqueeze(2)).sigmoid().log().sum(1).squeeze()\n",
    "        self.sampler = WalkerAlias(np.power(counter, power))\n",
    "        \n",
    "    def negative_sampling(self, shape):\n",
    "        return tensor(self.sampler.sample(shape=shape), dtype=torch.long, device=self.device)\n",
    "    \n",
    "    def forward(self, X, context):\n",
    "        batch_size, seq_len = X.size()\n",
    "        embedding = self.W(X)\n",
    "        pos_loss = self.log_loss((embedding * context).sum(2))\n",
    "\n",
    "        neg_samples = self.negative_sampling(shape=(batch_size, seq_len, self.num_neg))\n",
    "        neg_embedding = self.W(neg_samples)\n",
    "        neg_loss = self.log_loss((-neg_embedding * context.unsqueeze(2)).sum(3)).sum(2)\n",
    "        return -(pos_loss + neg_loss).sum()\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_path = '../../../../../data/processed/raw_wikitext-2.npy'\n",
    "# sentences = np.load(train_path)\n",
    "# batch_size = 100\n",
    "# min_freq = 1\n",
    "# device = 'cpu'\n",
    "\n",
    "# dataset = WikiDataset(sentences, batch_size, min_freq, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../../context2vec2/src/model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../../context2vec2/src/model.py\n",
    "\n",
    "from negative_sampling import NegativeSampling\n",
    "from torch.nn.init import kaiming_normal\n",
    "from torch import tensor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def create_embedding_layer(vocab_size, word_embed_size, pad_idx):\n",
    "    return nn.Embedding(num_embeddings=vocab_size,\n",
    "                                    embedding_dim=word_embed_size,\n",
    "                                    padding_idx=pad_idx)\n",
    "\n",
    "def create_rnn_layer(word_embed_size, hidden_size, n_layers, batch_first, layer_type=nn.LSTM):\n",
    "    return layer_type(input_size=word_embed_size,\n",
    "                               hidden_size=hidden_size,\n",
    "                               num_layers=n_layers,\n",
    "                               batch_first=batch_first) \n",
    "\n",
    "class Context2vec(nn.Module):\n",
    "    def __init__(self, vocab_size, counter, word_embed_size, hidden_size, n_layers, bidirectional, dropout,\n",
    "                 pad_idx, device, inference):\n",
    "\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.word_embed_size = word_embed_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.device = device\n",
    "        self.inference = inference\n",
    "        self.rnn_output_size = hidden_size\n",
    "        \n",
    "        # embedding\n",
    "        self.left2right_embed = create_embedding_layer(vocab_size, word_embed_size, pad_idx)\n",
    "        self.right2left_embed = create_embedding_layer(vocab_size, word_embed_size, pad_idx)\n",
    "        for embed in [self.left2right_embed, self.right2left_embed]:\n",
    "            init_embeddings(embed)\n",
    "        # rnn\n",
    "        self.left2right_rnn = create_rnn_layer(word_embed_size, word_embed_size, n_layers, batch_first=True)\n",
    "        self.right2left_rnn = create_rnn_layer(word_embed_size, word_embed_size, n_layers, batch_first=True)\n",
    "        # dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # loss\n",
    "        self.neg_sample_loss = NegativeSampling(hidden_size, counter, pad_idx=pad_idx, num_neg=10, power=0.75,\n",
    "                                          device=device) # num_neg=10, power=0.75 used in paper\n",
    "        \n",
    "        self.top_model = NeuralNet(input_size=hidden_size*2, mid_size=hidden_size*2, output_size=hidden_size,\n",
    "                                                               dropout=dropout)\n",
    "        \n",
    "    def forward(self, X, y, target_pos=None):\n",
    "        batch_size, seq_len = X.size()\n",
    "        X_reversed = X.flip(1)[:, :-1]\n",
    "        X = X[:, :-1]\n",
    "        \n",
    "        left2right_embed = self.left2right_embed(X)\n",
    "        right2left_embed = self.right2left_embed(X_reversed)\n",
    "        \n",
    "        left2right_out, _ = self.left2right_rnn(left2right_embed)\n",
    "        right2left_out, _ = self.right2left_rnn(right2left_embed)\n",
    "        \n",
    "        left2right_out = left2right_out[:, :-1, :]\n",
    "        right2left_out = right2left_out[:, :-1, :].flip(1)\n",
    "        # TESTING\n",
    "        if self.inference:\n",
    "            left2right_out = left2right_out[0, target_pos]\n",
    "            right2left_out = right2left_out[0, target_pos]\n",
    "            out = self.top_model(torch.cat((left2right_out, right2left_out), dim=0))\n",
    "            return out\n",
    "        # TRAINING \n",
    "        else:\n",
    "            out = self.top_model(torch.cat((left2right_out, right2left_out), dim=2)) # dim = 2\n",
    "            loss = self.neg_sample_loss(y, out)\n",
    "            return loss \n",
    "        \n",
    "    def run_inference(self, input_tokens, target, target_pos, k=10):\n",
    "        context_vector = self.forward(input_tokens, target=None, target_pos=target_pos)\n",
    "        if target is None:\n",
    "            topv, topi = ((self.neg_sample_loss.W.weight*context_vector).sum(dim=1)).data.topk(k)\n",
    "            return topv, topi\n",
    "        else:\n",
    "            context_vector /= torch.norm(context_vector, p=2)\n",
    "            target_vector = self.neg_sample_loss.W.weight[target]\n",
    "            target_vector /= torch.norm(target_vector, p=2)\n",
    "            similarity = (target_vector * context_vector).sum()\n",
    "            return similarity.item()\n",
    "        \n",
    "        \n",
    "        \n",
    "class NeuralNet(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, mid_size, output_size, n_layers=2, dropout=0.3, activation_function='relu'):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.mid_size = mid_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "        self.MLP = nn.ModuleList()\n",
    "        if n_layers == 1:\n",
    "            self.MLP.append(nn.Linear(input_size, output_size))\n",
    "        else:\n",
    "            self.MLP.append(nn.Linear(input_size, mid_size))\n",
    "            for _ in range(n_layers - 2):\n",
    "                self.MLP.append(nn.Linear(mid_size, mid_size))\n",
    "            self.MLP.append(nn.Linear(mid_size, output_size))\n",
    "\n",
    "        if activation_function == 'tanh':\n",
    "            self.activation_function = nn.Tanh()\n",
    "        elif activation_function == 'relu':\n",
    "            self.activation_function = nn.ReLU()\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "        for i in range(self.n_layers-1):\n",
    "            out = self.MLP[i](self.drop(out))\n",
    "            out = self.activation_function(out)\n",
    "        return self.MLP[-1](self.drop(out))\n",
    "    \n",
    "    \n",
    "# class NeuralNet(nn.Module):\n",
    "#     def __init__(self, out_sz, sizes, drops, y_range=None, use_bn=False, f=F.relu)\n",
    "#     def __init__(self, input_size, mid_size, output_size, dropout):\n",
    "#         super().__init__()\n",
    "        \n",
    "#         self.linear = nn.ModuleList([nn.Linear(sizes[i], sizes[i+1]) for i in range(len(sizes)-1)])\n",
    "#         self.bns = nn.ModuleList([nn.BatchNorm1d(size) for size in sizes[1:]])\n",
    "#         for layer in self.linear:\n",
    "#             kaiming_normal(layer.weight.data)\n",
    "#         self.dropout = [nn.Dropout(drop) for drop in drops]\n",
    "#         self.output = nn.Linear(sizes[-1], 1)\n",
    "#         kaiming_normal(self.output.weight.data)\n",
    "#         self.f = f\n",
    "#         self.use_bn = use_bn\n",
    "            \n",
    "        \n",
    "#     def forward(self, X):\n",
    "#         for linear, drop, norm in zip(self.linear, self.dropout, self.bns):\n",
    "#             X = self.f(linear(X))\n",
    "#             if self.use_bn: \n",
    "#                 X = norm(X)\n",
    "#             X = drop(X)\n",
    "#         X = self.output(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.init import kaiming_normal\n",
    "\n",
    "\n",
    "class NeuralNet(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, mid_size, output_size, n_layers=2, dropout=0.3, activation_function='relu'):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.mid_size = mid_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "        self.MLP = nn.ModuleList()\n",
    "        if n_layers == 1:\n",
    "            self.MLP.append(nn.Linear(input_size, output_size))\n",
    "        else:\n",
    "            self.MLP.append(nn.Linear(input_size, mid_size))\n",
    "            for _ in range(n_layers - 2):\n",
    "                self.MLP.append(nn.Linear(mid_size, mid_size))\n",
    "            self.MLP.append(nn.Linear(mid_size, output_size))\n",
    "\n",
    "        if activation_function == 'tanh':\n",
    "            self.activation_function = nn.Tanh()\n",
    "        elif activation_function == 'relu':\n",
    "            self.activation_function = nn.ReLU()\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "        for i in range(self.n_layers-1):\n",
    "            out = self.MLP[i](self.drop(out))\n",
    "            out = self.activation_function(out)\n",
    "        return self.MLP[-1](self.drop(out))\n",
    "    \n",
    "    \n",
    "# class NeuralNet(nn.Module):\n",
    "#     def __init__(self, out_sz, sizes, drops, y_range=None, use_bn=False, f=F.relu)\n",
    "#     def __init__(self, input_size, mid_size, output_size, dropout):\n",
    "#         super().__init__()\n",
    "        \n",
    "#         self.linear = nn.ModuleList([nn.Linear(sizes[i], sizes[i+1]) for i in range(len(sizes)-1)])\n",
    "#         self.bns = nn.ModuleList([nn.BatchNorm1d(size) for size in sizes[1:]])\n",
    "#         for layer in self.linear:\n",
    "#             kaiming_normal(layer.weight.data)\n",
    "#         self.dropout = [nn.Dropout(drop) for drop in drops]\n",
    "#         self.output = nn.Linear(sizes[-1], 1)\n",
    "#         kaiming_normal(self.output.weight.data)\n",
    "#         self.f = f\n",
    "#         self.use_bn = use_bn\n",
    "            \n",
    "        \n",
    "#     def forward(self, X):\n",
    "#         for linear, drop, norm in zip(self.linear, self.dropout, self.bns):\n",
    "#             X = self.f(linear(X))\n",
    "#             if self.use_bn: \n",
    "#                 X = norm(X)\n",
    "#             X = drop(X)\n",
    "#         X = self.output(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../../context2vec2/main.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../../context2vec2/main.py\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "from torch import optim\n",
    "from src.mscc_eval import mscc_evaluation\n",
    "from src.model import Context2vec\n",
    "# from src.util.args import parse_args\n",
    "from src.dataset import WikiDataset\n",
    "# from src.util.config import Config\n",
    "# from src.util.io import write_embedding, write_config, read_config, load_vocab\n",
    "import boto3\n",
    "from io import BytesIO\n",
    "\n",
    "\n",
    "array_file = True  \n",
    "text_file = False\n",
    "use_s3 = False\n",
    "use_validation_set = True\n",
    "# validation_data = '../../../../data/processed/rawwikitext-2-valid.npy'\n",
    "validation_data = '../../../../../data/processed/rawwikitext-103-valid.npy'\n",
    "S3_BUCKET = 'handwrittingdetection'\n",
    "S3_WIKI_TRAIN_PATH = 'data/wiki_train/rawwikitext-2-train.npy'\n",
    "S3_WIKI_VAL_PATH = 'data/wiki_valid/rawwikitext-2-valid.npy'\n",
    "log_dir_name = 'logs'\n",
    "log_filename = 'log_dir1.txt'\n",
    "\n",
    "\n",
    "\n",
    "train = True\n",
    "word_embed_size = 300\n",
    "hidden_size = 300\n",
    "n_layers = 1\n",
    "dropout = 0.00\n",
    "n_epochs = 1\n",
    "batch_size = 100\n",
    "min_freq = 1\n",
    "ns_power = 0.75\n",
    "learning_rate = 1e-4\n",
    "gpu_id = 0\n",
    "\n",
    "def main(train_path):\n",
    "#     use_cuda = torch.cuda.is_available()\n",
    "    use_cuda = torch.cuda.is_available() and gpu_id > -1\n",
    "    max_sent_length = 64\n",
    "    if use_cuda:\n",
    "        device = torch.device('cuda:{}'.format(gpu_id))\n",
    "        torch.cuda.set_device(gpu_id)\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "\n",
    "    if train:\n",
    "#         batch_size = batch_size\n",
    "#         n_epochs = n_epochs\n",
    "#         word_embed_size = word_embed_size\n",
    "#         hidden_size = hidden_size\n",
    "#         learning_rate = learning_rate\n",
    "\n",
    "        \n",
    "        if use_s3:\n",
    "            print('Loading Training Data from S3 bucket {} -- {}'.format(S3_BUCKET, S3_WIKI_TRAIN_PATH))\n",
    "            client = boto3.resource('s3')\n",
    "            bucket = client.Bucket(S3_BUCKET)\n",
    "            sentences = np.load(BytesIO(bucket.Object(S3_WIKI_TRAIN_PATH).get()['Body'].read()))\n",
    "        else:\n",
    "            sentences = np.load(train_path)\n",
    "            \n",
    "        \n",
    "        \n",
    "        print('Creating dataset')\n",
    "        dataset = WikiDataset(sentences, batch_size, min_freq, device)\n",
    "        counter = np.array([dataset.vocab.freqs[word] if word in dataset.vocab.freqs else 0\n",
    "                            for word in dataset.vocab.itos])\n",
    "        model = Context2vec(vocab_size=len(dataset.vocab),\n",
    "                            counter=counter,\n",
    "                            word_embed_size=word_embed_size,\n",
    "                            hidden_size=hidden_size,\n",
    "                            n_layers=n_layers,\n",
    "                            bidirectional=True,\n",
    "                            dropout=dropout,\n",
    "                            pad_idx=dataset.pad_idx,\n",
    "                            device=device,\n",
    "                            inference=False).to(device)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "        print('batch_size:{}, n_epochs:{}, word_embed_size:{}, hidden_size:{}, device:{}'.format(\n",
    "                                                batch_size, n_epochs, word_embed_size, hidden_size, device))\n",
    "        print(model)\n",
    "        \n",
    "        if use_validation_set:\n",
    "            if use_s3:\n",
    "                print('Loading Validation Data from S3 bucket {} -- {}'.format(S3_BUCKET, S3_WIKI_VAL_PATH))\n",
    "                val_sentences = np.load(BytesIO(bucket.Object(S3_WIKI_VAL_PATH).get()['Body'].read()))\n",
    "            else:\n",
    "                val_sentences = np.load(validation_data)\n",
    "            \n",
    "            print('Creating Validation dataset')\n",
    "            val_dataset = WikiDataset(val_sentences, batch_size, min_freq, device)\n",
    "            val_counter = np.array([val_dataset.vocab.freqs[word] if word in val_dataset.vocab.freqs else 0\n",
    "                                for word in val_dataset.vocab.itos])\n",
    "            \n",
    "        log_dir = os.path.dirname(log_dir_name)\n",
    "        if log_dir and not os.path.exists(log_dir):\n",
    "            os.makedirs(log_dir)\n",
    "            \n",
    "            \n",
    "        best_val_score = float('inf')\n",
    "        print('Training Begins')\n",
    "        interval = 1e6\n",
    "        for epoch in range(n_epochs):\n",
    "            begin_time = time.time()\n",
    "            cur_at = begin_time\n",
    "            total_loss = 0.0\n",
    "            val_total_loss = 0.0\n",
    "            word_count = 0\n",
    "            next_count = interval\n",
    "            last_accum_loss = 0.0\n",
    "            last_word_count = 0\n",
    "            \n",
    "            model.train() \n",
    "            for iterator in dataset.get_batch_iter(batch_size):\n",
    "                for batch in iterator:\n",
    "                    sentence = getattr(batch, 'sentence')\n",
    "                    target = sentence[:, 1:-1]\n",
    "                    if target.size(0) == 0:\n",
    "                        continue\n",
    "                    optimizer.zero_grad()\n",
    "                    loss = model(sentence, target)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    total_loss += loss.data.mean()\n",
    "\n",
    "                    minibatch_size, sentence_length = target.size()\n",
    "                    word_count += minibatch_size * sentence_length\n",
    "                    accum_mean_loss = float(total_loss)/word_count if total_loss > 0.0 else 0.0\n",
    "                    if word_count >= next_count:\n",
    "                        now = time.time()\n",
    "                        duration = now - cur_at\n",
    "                        throuput = float((word_count-last_word_count)) / (now - cur_at)\n",
    "                        cur_mean_loss = (float(total_loss)-last_accum_loss)/(word_count-last_word_count)\n",
    "                        print('{} words, {:.2f} sec, {:.2f} words/sec, {:.4f} accum_loss/word, {:.4f} cur_loss/word'\n",
    "                              .format(word_count, duration, throuput, accum_mean_loss, cur_mean_loss))\n",
    "                        next_count += interval\n",
    "                        cur_at = now\n",
    "                        last_accum_loss = float(total_loss)\n",
    "                        last_word_count = word_count\n",
    "\n",
    "\n",
    "            \n",
    "            # ---------\n",
    "            # VAL PHASE\n",
    "            model.eval()\n",
    "            for val_iterator in val_dataset.get_batch_iter(batch_size):\n",
    "                with torch.no_grad():\n",
    "                    for batch in val_iterator:\n",
    "                        val_sentence = getattr(batch, 'sentence')\n",
    "                        val_target = val_sentence[:, 1:-1]\n",
    "                        if val_target.size(0) == 0:\n",
    "                            continue\n",
    "                        val_loss = model(val_sentence, val_target)\n",
    "                        val_total_loss += val_loss.data.mean()\n",
    "            print('Train loss: {} -- Valid loss: {}'.format(total_loss.item(), val_total_loss.item()))\n",
    "            print()\n",
    "            \n",
    "    \n",
    "        # ---------\n",
    "            print(os.path.join(log_dir_name, log_filename))\n",
    "            print(log_dir_name + '/' + log_filename)\n",
    "            with open(os.path.join(log_dir_name, log_filename), 'a') as f:\n",
    "                f.write(str(epoch) + ' ' + str(total_loss.item()) + ' ' + str(val_total_loss.item()) + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dataset\n",
      "batch_size:100, n_epochs:1, word_embed_size:300, hidden_size:300, device:cuda:0\n",
      "Context2vec(\n",
      "  (left2right_embed): Embedding(33201, 300, padding_idx=1)\n",
      "  (right2left_embed): Embedding(33201, 300, padding_idx=1)\n",
      "  (left2right_rnn): LSTM(300, 300, batch_first=True)\n",
      "  (right2left_rnn): LSTM(300, 300, batch_first=True)\n",
      "  (dropout): Dropout(p=0.0)\n",
      "  (neg_sample_loss): NegativeSampling(\n",
      "    (W): Embedding(33201, 300, padding_idx=1)\n",
      "    (log_loss): LogSigmoid()\n",
      "  )\n",
      "  (top_model): NeuralNet(\n",
      "    (drop): Dropout(p=0.0)\n",
      "    (MLP): ModuleList(\n",
      "      (0): Linear(in_features=600, out_features=600, bias=True)\n",
      "      (1): Linear(in_features=600, out_features=300, bias=True)\n",
      "    )\n",
      "    (activation_function): ReLU()\n",
      "  )\n",
      ")\n",
      "Creating Validation dataset\n",
      "Training Begins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matt/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:122: DeprecationWarning: generator 'Iterator.__iter__' raised StopIteration\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000180 words, 8.02 sec, 124633.15 words/sec, 3.7575 accum_loss/word, 3.7575 cur_loss/word\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matt/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:156: DeprecationWarning: generator 'Iterator.__iter__' raised StopIteration\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 6812877.0 -- Valid loss: 5959606.0\n",
      "\n",
      "logs/log_dir1.txt\n",
      "logs/log_dir1.txt\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'logs/log_dir1.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-e546273978dc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrain_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'../../../../../data/processed/rawwikitext-103-valid.npy'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-20-45f61d45b00b>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(train_path)\u001b[0m\n\u001b[1;32m    168\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_dir_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_dir_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlog_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_dir_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'a'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_total_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'logs/log_dir1.txt'"
     ]
    }
   ],
   "source": [
    "train_path = '../../../../../data/processed/rawwikitext-103-valid.npy'\n",
    "main(train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls ../../../../../data/processed/rawwikitext-103-valid.npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
