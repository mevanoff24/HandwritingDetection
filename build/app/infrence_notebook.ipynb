{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lm_preds [(0.8051942586898804, 'into'), (0.5680796504020691, 'above'), (0.2566736936569214, 'through'), (0.0, '<PAD>'), (0.0, '<BOS>'), (0.0, '<EOS>'), (-0.0033715423196554184, 'against'), (-0.0241890586912632, 'across'), (-0.09841558337211609, 'alongside'), (-0.25158366560935974, 'from'), (-0.2979653775691986, '<UNK>'), (-0.39155226945877075, 'onto'), (-0.42664194107055664, '-'), (-0.5151805281639099, 'along'), (-0.5289998650550842, 'down'), (-0.6323362588882446, 'past'), (-0.6823214888572693, 'beside'), (-0.7862836718559265, 'behind'), (-0.7909708023071289, 'to'), (-0.8059641718864441, 'off'), (-0.8149632811546326, 'throughout'), (-0.8522829413414001, 'towards'), (-0.8726194500923157, 'near'), (-1.0634551048278809, 'beyond'), (-1.0821455717086792, 'inhabiting'), (-1.1535910367965698, 'woven'), (-1.1612988710403442, 'under'), (-1.1739630699157715, 'Hugh'), (-1.2416491508483887, 'wars'), (-1.327302098274231, 'inside')]\n",
      "ocr_pred ['o']\n",
      "into\n"
     ]
    }
   ],
   "source": [
    "# LM model imports \n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "from torch import optim\n",
    "from models.context2vec.src.eval.mscc import mscc_evaluation\n",
    "from models.context2vec.src.core.nets import Context2vec\n",
    "from models.context2vec.src.util.args import parse_args\n",
    "from models.context2vec.src.util.batch import Dataset\n",
    "from models.context2vec.src.util.config import Config\n",
    "from models.context2vec.src.util.io import write_embedding, write_config, read_config, load_vocab\n",
    "\n",
    "\n",
    "# OCR model imports \n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "import cv2\n",
    "import itertools\n",
    "from models.ocr.src.config import letters\n",
    "\n",
    "\n",
    "# TODO \n",
    "# -- need to add if the model is not loaded -- error print statement \n",
    "\n",
    "\n",
    "# LM model imports \n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "from torch import optim\n",
    "from models.context2vec.src.eval.mscc import mscc_evaluation\n",
    "from models.context2vec.src.core.nets import Context2vec\n",
    "from models.context2vec.src.util.args import parse_args\n",
    "from models.context2vec.src.util.batch import Dataset\n",
    "from models.context2vec.src.util.config import Config\n",
    "from models.context2vec.src.util.io import write_embedding, write_config, read_config, load_vocab\n",
    "\n",
    "\n",
    "# OCR model imports \n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "import cv2\n",
    "import itertools\n",
    "from models.ocr.src.config import letters\n",
    "\n",
    "from collections import defaultdict\n",
    "from operator import itemgetter\n",
    "\n",
    "\n",
    "\n",
    "class Inference():\n",
    "    def __init__(self, modelfile=None, wordsfile=None, img_width=128, img_height=64, device='cpu'):\n",
    "        \n",
    "        self.device = device\n",
    "        self.img_width = img_width\n",
    "        self.img_height = img_height\n",
    "        self.build_language_model()\n",
    "        self.build_ocr_model()\n",
    "        \n",
    "    def build_language_model(self):\n",
    "        # LANGUAGE MODEL\n",
    "        modelfile = 'models/context2vec/models/model.param'\n",
    "        wordsfile = 'models/context2vec/models/embedding.vec'\n",
    "        config_file = modelfile+'.config.json'\n",
    "        config_dict = read_config(config_file)\n",
    "        self.lm_model = Context2vec(vocab_size=config_dict['vocab_size'],\n",
    "                            counter=[1]*config_dict['vocab_size'],\n",
    "                            word_embed_size=config_dict['word_embed_size'],\n",
    "                            hidden_size=config_dict['hidden_size'],\n",
    "                            n_layers=config_dict['n_layers'],\n",
    "                            bidirectional=config_dict['bidirectional'],\n",
    "                            use_mlp=config_dict['use_mlp'],\n",
    "                            dropout=config_dict['dropout'],\n",
    "                            pad_index=config_dict['pad_index'],\n",
    "                            device=self.device,\n",
    "                            inference=True).to(self.device)\n",
    "        self.lm_model.load_state_dict(torch.load(modelfile, map_location=self.device))\n",
    "#         optimizer = optim.Adam(model.parameters(), lr=config_dict['learning_rate'])\n",
    "        # optimizer.load_state_dict(torch.load(modelfile+'.optim'))\n",
    "        self.itos, self.stoi = load_vocab(wordsfile)\n",
    "        self.unk_token = config_dict['unk_token']\n",
    "        self.bos_token = config_dict['bos_token']\n",
    "        self.eos_token = config_dict['eos_token']\n",
    "\n",
    "        \n",
    "    def build_ocr_model(self):\n",
    "        self.sess = tf.Session()\n",
    "        K.set_session(self.sess)\n",
    "\n",
    "        ocr_model_path = 'models/ocr/models/weights-improvement2-10-01-3.00.hdf5'\n",
    "        self.ocr_model = load_model(ocr_model_path, custom_objects={'<lambda>': lambda y_true, y_pred: y_pred})\n",
    "        \n",
    "    def preprocess_image(self, img_path, img_width, img_height):\n",
    "        img = cv2.imread(img_path)\n",
    "        # grayscale image\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        # resize image\n",
    "        img = cv2.resize(img, (img_width, img_height))\n",
    "        # change image type\n",
    "        img = img.astype(np.float32)\n",
    "        # scale image \n",
    "        img /= 255\n",
    "        img = img.reshape((1, img_width, img_height, 1))\n",
    "        return img\n",
    "        \n",
    "        \n",
    "    def _decode_batch(self, out):\n",
    "        ret = []\n",
    "        for j in range(out.shape[0]):\n",
    "            out_best = list(np.argmax(out[j, 2:], 1))\n",
    "            out_best = [k for k, g in itertools.groupby(out_best)]\n",
    "            outstr = ''\n",
    "            for c in out_best:\n",
    "                if c < len(letters):\n",
    "                    outstr += letters[c]\n",
    "            ret.append(outstr)\n",
    "        return ret\n",
    "\n",
    "        \n",
    "    def _return_split_sentence(self, sentence):\n",
    "        if ' ' not in sentence:\n",
    "            print('sentence should contain white space to split it into tokens')\n",
    "            raise SyntaxError\n",
    "        elif '[]' not in sentence:\n",
    "            print('sentence should contain `[]` that notes the target')\n",
    "            raise SyntaxError\n",
    "        else:\n",
    "            tokens = sentence.lower().strip().split()\n",
    "            target_pos = tokens.index('[]')\n",
    "            return tokens, target_pos\n",
    "        \n",
    "    def run_lm_inference_by_user_input(self, sentence, topK=30):\n",
    "\n",
    "        # evaluation mode \n",
    "        self.lm_model.eval()\n",
    "        # norm_weight\n",
    "        self.lm_model.norm_embedding_weight(self.lm_model.criterion.W)\n",
    "\n",
    "        tokens, target_pos = self._return_split_sentence(sentence)\n",
    "        tokens[target_pos] = self.unk_token\n",
    "        tokens = [self.bos_token] + tokens + [self.eos_token]\n",
    "        indexed_sentence = [self.stoi[token] if token in self.stoi else self.stoi[self.unk_token] for token in tokens]\n",
    "        input_tokens = \\\n",
    "            torch.tensor(indexed_sentence, dtype=torch.long, device=self.device).unsqueeze(0)\n",
    "        topv, topi = self.lm_model.run_inference(input_tokens, target=None, target_pos=target_pos, k=topK)\n",
    "        output = []  \n",
    "        for value, key in zip(topv, topi):\n",
    "            output.append((value.item(), self.itos[key.item()]))\n",
    "#             print(value.item(), self.itos[key.item()])\n",
    "        return output\n",
    "    \n",
    "    def run_ocr_inference_by_user_image(self, img):\n",
    "        net_inp = self.ocr_model.get_layer(name='the_input').input\n",
    "        net_out = self.ocr_model.get_layer(name='softmax').output\n",
    "        net_out_value = self.sess.run(net_out, feed_dict={net_inp: img})\n",
    "        pred_texts = self._decode_batch(net_out_value)\n",
    "        return pred_texts\n",
    "    \n",
    "    from collections import defaultdict\n",
    "    from operator import itemgetter\n",
    "\n",
    "\n",
    "    def create_features(self, lm_preds, ocr_pred):\n",
    "\n",
    "        # not used currently\n",
    "        # ----------------------------------\n",
    "        bins = {\n",
    "            'small': list(range(0, 3)),\n",
    "            'small-mid': list(range(2, 6)),\n",
    "            'mid': list(range(4, 8)),\n",
    "            'mid-large': list(range(6, 10)),\n",
    "            'large': list(range(8, 12)),\n",
    "            'large-big': list(range(10, 14)),\n",
    "            'big': list(range(12, 100)),\n",
    "        }\n",
    "\n",
    "        bins = defaultdict(lambda: 'na', bins)\n",
    "\n",
    "        ocr_len = len([x for x in ocr_pred[0]])\n",
    "        pred_bins = [k for k, v in bins.items() if ocr_len in v]\n",
    "        # ----------------------------------\n",
    "\n",
    "        features = {}\n",
    "        bad_list = ['<PAD>', '<BOS>', '<EOS>', '<UNK>'] # ADD foul words\n",
    "        matches = {}\n",
    "        matches_non_ordered = {}\n",
    "\n",
    "        ocr_pred_lower = ocr_pred[0].lower()\n",
    "\n",
    "        for lm_pred in lm_preds:\n",
    "            score, word = lm_pred[0], lm_pred[1].rstrip()\n",
    "            word = word.lower()\n",
    "            # remove pad, bos, etc...\n",
    "            if word not in bad_list:\n",
    "                try:\n",
    "                    features[word] = {}\n",
    "                    features[word]['score'] = score\n",
    "                    # match first and last character \n",
    "                    first_char_match = word[0] == ocr_pred_lower[0]\n",
    "                    last_char_match = word[-1] == ocr_pred_lower[-1]\n",
    "                    features[word]['first_char_match'] = first_char_match\n",
    "                    features[word]['last_char_match'] = last_char_match\n",
    "\n",
    "                    num_chars = 0\n",
    "                    for char in ocr_pred_lower:\n",
    "                        if char in word:\n",
    "                            num_chars += 1\n",
    "                        matches[word] = num_chars\n",
    "                    features[word]['num_matches'] = matches[word] \n",
    "                except:\n",
    "                    pass \n",
    "\n",
    "        return features\n",
    "\n",
    "\n",
    "    def final_scores(self, features):\n",
    "        final_scores = {}\n",
    "\n",
    "        for word, feature_dict in features.items():\n",
    "            final_score = 1\n",
    "            first_char_match = feature_dict['first_char_match']\n",
    "            last_char_match = feature_dict['last_char_match']\n",
    "            score = feature_dict['score']\n",
    "            if first_char_match:\n",
    "                final_score += 10\n",
    "            if last_char_match:\n",
    "                final_score += 10\n",
    "            final_score *= score\n",
    "            final_scores[word] = final_score\n",
    "        top_results = sorted(final_scores.items(), key=itemgetter(1), reverse=True)\n",
    "        return top_results[0][0]\n",
    "\n",
    "    def weigh_function(self):\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def predict(self, sentence, img_path):\n",
    "        lm_preds = self.run_lm_inference_by_user_input(sentence)\n",
    "        print('lm_preds', lm_preds)\n",
    "        img = self.preprocess_image(img_path, self.img_width, self.img_height)\n",
    "        ocr_pred = self.run_ocr_inference_by_user_image(img)\n",
    "        print('ocr_pred', ocr_pred)\n",
    "\n",
    "        features = self.create_features(lm_preds, ocr_pred)\n",
    "        final_pred = self.final_scores(features)\n",
    "    \n",
    "        # return top K? And use MAP @ K ??\n",
    "        return final_pred\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    left_text = 'the dog ran'\n",
    "    right_text = 'the house'\n",
    "\n",
    "    sentence = left_text + ' [] ' + right_text\n",
    "\n",
    "    img_width = 128\n",
    "    img_height = 64\n",
    "    img_path = '../../data/sample/c03-096f-03-05.png'\n",
    "\n",
    "\n",
    "    inference = Inference()\n",
    "\n",
    "    print(inference.predict(sentence, img_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=L size=208x107 at 0x7F2410251BE0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "Image.open('../../data/sample/c03-096f-03-05.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.4.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
