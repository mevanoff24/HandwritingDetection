{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION = 'us-west2'\n",
    "\n",
    "import os\n",
    "os.environ['key'] = key\n",
    "os.environ['secret'] = secret\n",
    "os.environ['REGION'] = REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "AWS_SERVER_PUBLIC_KEY = os.environ['key']\n",
    "\n",
    "AWS_SERVER_SECRET_KEY = os.environ['secret']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dill as pickle\n",
    "import os\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import pytesseract\n",
    "from PIL import Image, ImageEnhance, ImageFilter\n",
    "from collections import defaultdict\n",
    "import shutil\n",
    "\n",
    "\n",
    "# TODO remove this \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "\n",
    "# from .utils import unpickle\n",
    "# TODO Move to utils \n",
    "def unpickle(filename):\n",
    "    \"\"\" Unpickle file \"\"\"\n",
    "    with open(filename, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "\n",
    "# DATA_DIR = '../../../data'\n",
    "\n",
    "DATA_DIR = '../../data'\n",
    "# TODO move this \n",
    "data_path = os.path.join(DATA_DIR, 'raw/word_level')\n",
    "meta_json_data_path = os.path.join(DATA_DIR, 'preprocessed/meta.json')\n",
    "word_level_meta_path = os.path.join(DATA_DIR, 'preprocessed/word_level_meta.csv')\n",
    "word_path_mapping_path = os.path.join(DATA_DIR, 'processed/word_path_mapping.pkl')\n",
    "X_path = os.path.join(DATA_DIR, 'processed/X.npy')\n",
    "y_path = os.path.join(DATA_DIR, 'processed/y.npy')\n",
    "bigram_model_path = os.path.join(DATA_DIR, 'processed/ngram_models/bigram_likelihood_model.pkl')\n",
    "trigram_model_path = os.path.join(DATA_DIR, 'processed/ngram_models/trigram_likelihood_model.pkl')\n",
    "letters_path = os.path.join(DATA_DIR, 'processed/letters_map.pkl')\n",
    "\n",
    "\n",
    "import boto3\n",
    "from io import BytesIO\n",
    "import dill as pickle\n",
    "\n",
    "\n",
    "# client = boto3.resource('s3')\n",
    "# bucket = client.Bucket('handwrittingdetection')\n",
    "\n",
    "# session = boto3.Session(\n",
    "#     aws_access_key_id=settings.AWS_SERVER_PUBLIC_KEY,\n",
    "#     aws_secret_access_key=settings.AWS_SERVER_SECRET_KEY,\n",
    "# )\n",
    "# client = session.resource('s3')\n",
    "\n",
    "# s3_client = boto3.client('s3', \n",
    "#                       aws_access_key_id=AWS_SERVER_PUBLIC_KEY, \n",
    "#                       aws_secret_access_key=AWS_SERVER_SECRET_KEY, \n",
    "#                       region_name='us-west2'\n",
    "#                       )\n",
    "\n",
    "\n",
    "def s3_init(bucketname='handwrittingdetection'):\n",
    "\n",
    "    \n",
    "    session = boto3.Session(\n",
    "        aws_access_key_id=AWS_SERVER_PUBLIC_KEY,\n",
    "        aws_secret_access_key=AWS_SERVER_SECRET_KEY,\n",
    "    )\n",
    "    \n",
    "    client = session.resource('s3')\n",
    "    bucket = client.Bucket(bucketname)\n",
    "    return client, bucket\n",
    "\n",
    "# client, bucket = s3_init(bucketname='handwrittingdetection')\n",
    "    \n",
    "def unpickle_s3(filename, client=None, bucket=None):\n",
    "    with BytesIO() as data:\n",
    "        bucket.download_fileobj(filename, data)\n",
    "        data.seek(0)\n",
    "        return pickle.load(data)\n",
    "\n",
    "\n",
    "def tokenize_and_join(context):\n",
    "    tokens = word_tokenize(context)\n",
    "    context = ' '.join(tokens)\n",
    "    return tokens, context\n",
    "\n",
    "def ngram_backoff_model(left_text, right_text, trigram_model, bigram_model, OOV_token=0):\n",
    "\n",
    "\n",
    "    left_tokens, left_text = tokenize_and_join(left_text)\n",
    "    right_tokens, right_text = tokenize_and_join(right_text)\n",
    "    full_text = left_text + ' [] '  + right_text\n",
    "\n",
    "    # get previous word(s)\n",
    "    try:\n",
    "        prev_word = left_tokens[-1]\n",
    "    except:\n",
    "        prev_word = '<bos>'\n",
    "    try:\n",
    "        prev_prev_word = left_tokens[-2]\n",
    "    except:\n",
    "        prev_prev_word = '<bos>'\n",
    "    # model preds \n",
    "#     print(prev_prev_word, prev_word)\n",
    "    pred = trigram_model[(prev_prev_word, prev_word)]\n",
    "#     print('pred1', pred)\n",
    "    if pred == OOV_token:\n",
    "        pred = bigram_model[(prev_word)]\n",
    "#         print('pred2', pred)\n",
    "        if pred == OOV_token:\n",
    "            pred = 'UNK'\n",
    "    return pred \n",
    "\n",
    "\n",
    "def teseract_baseline(file_url, word_path_mapping, letters, tmpdir='tmp/'):\n",
    "    img_width = 256\n",
    "    img_height = 100\n",
    "    # gets image from sample index\n",
    "    # img = word_path_mapping[y[sample_index][0]]\n",
    "    im = Image.open(file_url)  # img is the path of the image\n",
    "    im = im.convert(\"RGBA\")\n",
    "    im = im.resize((img_width, img_height))\n",
    "    newimdata = []\n",
    "    datas = im.getdata()\n",
    "\n",
    "    vals = 255\n",
    "\n",
    "    for item in datas:\n",
    "        if item[0] < vals or item[1] < vals or item[2] < vals:\n",
    "            newimdata.append(item)\n",
    "        else:\n",
    "            newimdata.append((255, 255, 255))\n",
    "    im.putdata(newimdata)\n",
    "\n",
    "    im = im.filter(ImageFilter.MedianFilter())\n",
    "    enhancer = ImageEnhance.Contrast(im)\n",
    "    im = enhancer.enhance(2)\n",
    "    im = im.convert('1')\n",
    "    \n",
    "    if not os.path.exists(tmpdir):\n",
    "        os.makedirs(tmpdir)\n",
    "    save_img_path = os.path.join(tmpdir, 'temp_img.jpg')\n",
    "    im.save(save_img_path)\n",
    "    \n",
    "    text = pytesseract.image_to_string(Image.open(save_img_path),\n",
    "                config='-c tessedit_char_whitelist=0123456789abcdefghijklmnopqrstuvwxyz -psm 13', lang='eng')\n",
    "    \n",
    "    # os.remove(save_img_path)\n",
    "    # os.rmdir(tmpdir)\n",
    "    # shutil.rmtree(tmpdir)\n",
    "    return text, len(text) \n",
    "\n",
    "\n",
    "    \n",
    "def get_ocr_model_pred(file_url, word_path_mapping, letters):\n",
    "    ocr_pred, len_pred = teseract_baseline(file_url, word_path_mapping, letters)\n",
    "    return ocr_pred, len_pred\n",
    "\n",
    "def get_language_model_pred(left_text, right_text, trigram_model, bigram_model):\n",
    "    pred = ngram_backoff_model(left_text, right_text, trigram_model, bigram_model)\n",
    "    return pred\n",
    "\n",
    "\n",
    "def get_pos_tags(left_text, right_text):\n",
    "    return []\n",
    "\n",
    "def weight_features(left_text, right_text, file_url, trigram_model, bigram_model, word_path_mapping, letters, weights={}):\n",
    "    ocr_pred, len_pred = get_ocr_model_pred(file_url, word_path_mapping, letters)\n",
    "    print('OCR', ocr_pred, len_pred)\n",
    "    lm_pred = get_language_model_pred(left_text, right_text, trigram_model, bigram_model)\n",
    "    print('LM', lm_pred)\n",
    "    pos_pred = get_pos_tags(left_text, right_text)\n",
    "    \n",
    "    return ocr_pred, len_pred, lm_pred\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def absoluteFilePaths(directory):\n",
    "    \"\"\"Walk filepaths\"\"\"\n",
    "    for dirpath,_,filenames in os.walk(directory):\n",
    "        for f in filenames:\n",
    "            yield os.path.join(dirpath, f)\n",
    "\n",
    "\n",
    "def create_image_path(df, data_path, use_s3=False, s3_image_path='data/word_level'):\n",
    "    \"\"\"Create dictionary for mapping of word to data path\"\"\"\n",
    "    if use_s3:\n",
    "        files = list(bucket.objects.filter(Prefix='data/word_level/sample'))\n",
    "        all_paths = [f.key for f in files if '.png' in f.key]\n",
    "    else:\n",
    "        all_paths = [i for i in absoluteFilePaths(data_path)]\n",
    "        \n",
    "    all_path_dict = defaultdict(lambda: 0, dict(zip(all_path_endings, all_paths)))\n",
    "    all_path_endings = [i.split('/')[-1].split('.')[0] for i in all_paths]\n",
    "    defaultdict(lambda: 0, dict(zip(all_path_endings, all_paths)))\n",
    "    df['image_path'] = df['image_name'].map(lambda x: all_path_dict[x])\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def get_prediction(left_text, right_text, file_url, use_s3=False):\n",
    "\n",
    "    if use_s3:\n",
    "\n",
    "        client, bucket = s3_init(bucketname='handwrittingdetection')\n",
    "\n",
    "        bigram_model_path = 'data/ngram_models/bigram_likelihood_model.pkl'\n",
    "        trigram_model_path = 'data/ngram_models/trigram_likelihood_model.pkl'\n",
    "\n",
    "        word_path_mapping_path = 'data/word_path_mapping.pkl'\n",
    "        letters_path = 'data/letters_map.pkl'\n",
    "\n",
    "        bigram_model = unpickle_s3(bigram_model_path, client, bucket)\n",
    "        trigram_model = unpickle_s3(trigram_model_path, client, bucket)\n",
    "\n",
    "        word_path_mapping = unpickle_s3(word_path_mapping_path, client, bucket)\n",
    "        letter2idx = unpickle_s3(letters_path, client, bucket)\n",
    "\n",
    "\n",
    "    else:\n",
    "        data_path = os.path.join(DATA_DIR, 'raw/word_level')\n",
    "        meta_json_data_path = os.path.join(DATA_DIR, 'preprocessed/meta.json')\n",
    "        word_level_meta_path = os.path.join(DATA_DIR, 'preprocessed/word_level_meta.csv')\n",
    "        word_path_mapping_path = os.path.join(DATA_DIR, 'processed/word_path_mapping.pkl')\n",
    "        bigram_model_path = os.path.join(DATA_DIR, 'processed/ngram_models/bigram_likelihood_model.pkl')\n",
    "        trigram_model_path = os.path.join(DATA_DIR, 'processed/ngram_models/trigram_likelihood_model.pkl')\n",
    "        letters_path = os.path.join(DATA_DIR, 'processed/letters_map.pkl')\n",
    "\n",
    "        # Language Models\n",
    "        bigram_model = unpickle(bigram_model_path)\n",
    "        trigram_model = unpickle(trigram_model_path)\n",
    "\n",
    "        # Meta\n",
    "        meta = pd.read_json(meta_json_data_path)\n",
    "        word_level_df = pd.read_csv(word_level_meta_path)\n",
    "        word_level_df = create_image_path(word_level_df, data_path)\n",
    "\n",
    "        # word_path_mapping = defaultdict(lambda: 0, dict(zip(word_level_df.token, word_level_df.image_path)))                              \n",
    "        # with open(os.path.join(DATA_DIR, 'processed', 'word_path_mapping.pkl'), 'wb') as f:\n",
    "        #     pickle.dump(word_path_mapping, f)\n",
    "\n",
    "        word_path_mapping = unpickle(word_path_mapping_path)\n",
    "        letter2idx = unpickle(letters_path)\n",
    "\n",
    "    letters = list(letter2idx.keys())\n",
    "    letters = ''.join(letters[1:-2])\n",
    "\n",
    "    left_text = left_text.rstrip()\n",
    "    right_text = right_text.rstrip()\n",
    "\n",
    "    ocr_pred, len_pred, lm_pred = weight_features(left_text, right_text, file_url, trigram_model, bigram_model, word_path_mapping, letters)\n",
    "    return ocr_pred, len_pred, lm_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OCR wk 2\n",
      "dog ran\n",
      "pred1 0\n",
      "pred2 a\n",
      "LM a\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('wk', 2, 'a')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "left_text = 'the dog ran'\n",
    "right_text = 'the house'\n",
    "file_path = '../../data/raw/word_level/a01/a01-007u/a01-007u-00-05.png'\n",
    "\n",
    "get_prediction(left_text, right_text, file_path, use_s3=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=L size=152x72 at 0x1115C9048>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image.open(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "client, bucket = s3_init(bucketname='handwrittingdetection')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file = list(bucket.objects.filter(Prefix='models/language_model/model.param'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[s3.ObjectSummary(bucket_name='handwrittingdetection', key='models/language_model/model.param'),\n",
       " s3.ObjectSummary(bucket_name='handwrittingdetection', key='models/language_model/model.param.config.json'),\n",
       " s3.ObjectSummary(bucket_name='handwrittingdetection', key='models/language_model/model.param.optim')]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "from torch import optim\n",
    "from models.context2vec.src.eval.mscc import mscc_evaluation\n",
    "from models.context2vec.src.core.nets import Context2vec\n",
    "from models.context2vec.src.util.args import parse_args\n",
    "from models.context2vec.src.util.batch import Dataset\n",
    "from models.context2vec.src.util.config import Config\n",
    "from models.context2vec.src.util.io import write_embedding, write_config, read_config, load_vocab\n",
    "device = 'cpu'\n",
    "# args = parse_args()\n",
    "modelfile = 'models/context2vec/models/model.param'\n",
    "# modelfile = model_file[0].key\n",
    "wordsfile = 'models/context2vec/models/embedding.vec'\n",
    "config_file = modelfile+'.config.json'\n",
    "config_dict = read_config(config_file)\n",
    "model = Context2vec(vocab_size=config_dict['vocab_size'],\n",
    "                    counter=[1]*config_dict['vocab_size'],\n",
    "                    word_embed_size=config_dict['word_embed_size'],\n",
    "                    hidden_size=config_dict['hidden_size'],\n",
    "                    n_layers=config_dict['n_layers'],\n",
    "                    bidirectional=config_dict['bidirectional'],\n",
    "                    use_mlp=config_dict['use_mlp'],\n",
    "                    dropout=config_dict['dropout'],\n",
    "                    pad_index=config_dict['pad_index'],\n",
    "                    device=device,\n",
    "                    inference=True).to(device)\n",
    "model.load_state_dict(torch.load(modelfile, map_location='cpu'))\n",
    "optimizer = optim.Adam(model.parameters(), lr=config_dict['learning_rate'])\n",
    "# optimizer.load_state_dict(torch.load(modelfile+'.optim'))\n",
    "itos, stoi = load_vocab(wordsfile)\n",
    "unk_token = config_dict['unk_token']\n",
    "bos_token = config_dict['bos_token']\n",
    "eos_token = config_dict['eos_token']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "from torch import optim\n",
    "from models.context2vec.src.eval.mscc import mscc_evaluation\n",
    "from models.context2vec.src.core.nets import Context2vec\n",
    "from models.context2vec.src.util.args import parse_args\n",
    "from models.context2vec.src.util.batch import Dataset\n",
    "from models.context2vec.src.util.config import Config\n",
    "from models.context2vec.src.util.io import write_embedding, write_config, read_config, load_vocab\n",
    "\n",
    "class Inference():\n",
    "    def __init__(self, modelfile, wordsfile, config_file, device='cpu'):\n",
    "        \n",
    "        # args = parse_args()\n",
    "        modelfile = 'models/context2vec/models/model.param'\n",
    "        # modelfile = model_file[0].key\n",
    "        wordsfile = 'models/context2vec/models/embedding.vec'\n",
    "        config_file = modelfile+'.config.json'\n",
    "        config_dict = read_config(config_file)\n",
    "        model = Context2vec(vocab_size=config_dict['vocab_size'],\n",
    "                            counter=[1]*config_dict['vocab_size'],\n",
    "                            word_embed_size=config_dict['word_embed_size'],\n",
    "                            hidden_size=config_dict['hidden_size'],\n",
    "                            n_layers=config_dict['n_layers'],\n",
    "                            bidirectional=config_dict['bidirectional'],\n",
    "                            use_mlp=config_dict['use_mlp'],\n",
    "                            dropout=config_dict['dropout'],\n",
    "                            pad_index=config_dict['pad_index'],\n",
    "                            device=device,\n",
    "                            inference=True).to(device)\n",
    "        model.load_state_dict(torch.load(modelfile, map_location='cpu'))\n",
    "#         optimizer = optim.Adam(model.parameters(), lr=config_dict['learning_rate'])\n",
    "        # optimizer.load_state_dict(torch.load(modelfile+'.optim'))\n",
    "        itos, stoi = load_vocab(wordsfile)\n",
    "        unk_token = config_dict['unk_token']\n",
    "        bos_token = config_dict['bos_token']\n",
    "        eos_token = config_dict['eos_token']\n",
    "        \n",
    "        \n",
    "    def _return_split_sentence(self, sentence):\n",
    "        if ' ' not in sentence:\n",
    "            print('sentence should contain white space to split it into tokens')\n",
    "            raise SyntaxError\n",
    "        elif '[]' not in sentence:\n",
    "            print('sentence should contain `[]` that notes the target')\n",
    "            raise SyntaxError\n",
    "        else:\n",
    "            tokens = sentence.lower().strip().split()\n",
    "            target_pos = tokens.index('[]')\n",
    "            return tokens, target_pos\n",
    "        \n",
    "    def run_inference_by_user_input(self, sentence, model, itos, stoi, unk_token, bos_token, eos_token, device, topK=30):\n",
    "\n",
    "        # evaluation mode \n",
    "        model.eval()\n",
    "        # norm_weight\n",
    "        model.norm_embedding_weight(model.criterion.W)\n",
    "\n",
    "        tokens, target_pos = return_split_sentence(sentence)\n",
    "        tokens[target_pos] = unk_token\n",
    "        tokens = [bos_token] + tokens + [eos_token]\n",
    "        indexed_sentence = [stoi[token] if token in stoi else stoi[unk_token] for token in tokens]\n",
    "        input_tokens = \\\n",
    "            torch.tensor(indexed_sentence, dtype=torch.long, device=device).unsqueeze(0)\n",
    "        topv, topi = model.run_inference(input_tokens, target=None, target_pos=target_pos, k=topK)\n",
    "        output = []  \n",
    "        for value, key in zip(topv, topi):\n",
    "            output.append((value.item(), itos[key.item()]))\n",
    "            print(value.item(), itos[key.item()])\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference_by_user_input(sentence, \n",
    "                                model,\n",
    "                                itos,\n",
    "                                stoi,\n",
    "                                unk_token,\n",
    "                                bos_token,\n",
    "                                eos_token,\n",
    "                                device):\n",
    "\n",
    "    def return_split_sentence(sentence):\n",
    "        if ' ' not in sentence:\n",
    "            print('sentence should contain white space to split it into tokens')\n",
    "            raise SyntaxError\n",
    "        elif '[]' not in sentence:\n",
    "            print('sentence should contain `[]` that notes the target')\n",
    "            raise SyntaxError\n",
    "        else:\n",
    "            tokens = sentence.lower().strip().split()\n",
    "            target_pos = tokens.index('[]')\n",
    "            return tokens, target_pos\n",
    "\n",
    "    ''' \n",
    "    norm_weight\n",
    "    '''\n",
    "    # evaluation mode \n",
    "    model.eval()\n",
    "    \n",
    "    model.norm_embedding_weight(model.criterion.W)\n",
    "\n",
    "    tokens, target_pos = return_split_sentence(sentence)\n",
    "    tokens[target_pos] = unk_token\n",
    "    tokens = [bos_token] + tokens + [eos_token]\n",
    "    indexed_sentence = [stoi[token] if token in stoi else stoi[unk_token] for token in tokens]\n",
    "    input_tokens = torch.tensor(indexed_sentence, dtype=torch.long, device=device).unsqueeze(0)\n",
    "    topv, topi = model.run_inference(input_tokens, target=None, target_pos=target_pos)\n",
    "    output = []  \n",
    "    for value, key in zip(topv, topi):\n",
    "        output.append((value.item(), itos[key.item()]))\n",
    "        print(value.item(), itos[key.item()])\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelfile = 'models/context2vec/models/model.param'\n",
    "wordsfile = 'models/context2vec/models/embedding.vec'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8051932454109192 into\n",
      "0.5680792927742004 above\n",
      "0.25667303800582886 through\n",
      "0.0 <PAD>\n",
      "0.0 <EOS>\n",
      "0.0 <BOS>\n",
      "-0.0033715590834617615 against\n",
      "-0.02418931946158409 across\n",
      "-0.09841619431972504 alongside\n",
      "-0.25158366560935974 from\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0.8051932454109192, 'into'),\n",
       " (0.5680792927742004, 'above'),\n",
       " (0.25667303800582886, 'through'),\n",
       " (0.0, '<PAD>'),\n",
       " (0.0, '<EOS>'),\n",
       " (0.0, '<BOS>'),\n",
       " (-0.0033715590834617615, 'against'),\n",
       " (-0.02418931946158409, 'across'),\n",
       " (-0.09841619431972504, 'alongside'),\n",
       " (-0.25158366560935974, 'from')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "left_text = 'the dog ran'\n",
    "right_text = 'the house'\n",
    "\n",
    "sentence = left_text + ' [] ' + right_text\n",
    "\n",
    "run_inference_by_user_input(sentence, model, itos, stoi, unk_token=unk_token, \n",
    "                            bos_token=bos_token, eos_token=eos_token, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.param             model.param.config.json\r\n"
     ]
    }
   ],
   "source": [
    "!ls models/context2vec/models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__init__.py      run.py           \u001b[1m\u001b[34mtemplates\u001b[m\u001b[m/       \u001b[1m\u001b[34mtmp\u001b[m\u001b[m/\r\n",
      "\u001b[1m\u001b[34mmodels\u001b[m\u001b[m/          \u001b[1m\u001b[34mstatic\u001b[m\u001b[m/          test_pipe.ipynb  utils.py\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LM MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "from torch import optim\n",
    "from models.context2vec.src.eval.mscc import mscc_evaluation\n",
    "from models.context2vec.src.core.nets import Context2vec\n",
    "from models.context2vec.src.util.args import parse_args\n",
    "from models.context2vec.src.util.batch import Dataset\n",
    "from models.context2vec.src.util.config import Config\n",
    "from models.context2vec.src.util.io import write_embedding, write_config, read_config, load_vocab\n",
    "device = 'cpu'\n",
    "# args = parse_args()\n",
    "modelfile = 'models/context2vec/models/model.param'\n",
    "# modelfile = model_file[0].key\n",
    "wordsfile = 'models/context2vec/models/embedding.vec'\n",
    "config_file = modelfile+'.config.json'\n",
    "config_dict = read_config(config_file)\n",
    "model = Context2vec(vocab_size=config_dict['vocab_size'],\n",
    "                    counter=[1]*config_dict['vocab_size'],\n",
    "                    word_embed_size=config_dict['word_embed_size'],\n",
    "                    hidden_size=config_dict['hidden_size'],\n",
    "                    n_layers=config_dict['n_layers'],\n",
    "                    bidirectional=config_dict['bidirectional'],\n",
    "                    use_mlp=config_dict['use_mlp'],\n",
    "                    dropout=config_dict['dropout'],\n",
    "                    pad_index=config_dict['pad_index'],\n",
    "                    device=device,\n",
    "                    inference=True).to(device)\n",
    "model.load_state_dict(torch.load(modelfile, map_location='cpu'))\n",
    "optimizer = optim.Adam(model.parameters(), lr=config_dict['learning_rate'])\n",
    "# optimizer.load_state_dict(torch.load(modelfile+'.optim'))\n",
    "itos, stoi = load_vocab(wordsfile)\n",
    "unk_token = config_dict['unk_token']\n",
    "bos_token = config_dict['bos_token']\n",
    "eos_token = config_dict['eos_token']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference_by_user_input(sentence, \n",
    "                                model,\n",
    "                                itos,\n",
    "                                stoi,\n",
    "                                unk_token,\n",
    "                                bos_token,\n",
    "                                eos_token,\n",
    "                                device):\n",
    "\n",
    "    def return_split_sentence(sentence):\n",
    "        if ' ' not in sentence:\n",
    "            print('sentence should contain white space to split it into tokens')\n",
    "            raise SyntaxError\n",
    "        elif '[]' not in sentence:\n",
    "            print('sentence should contain `[]` that notes the target')\n",
    "            raise SyntaxError\n",
    "        else:\n",
    "            tokens = sentence.lower().strip().split()\n",
    "            target_pos = tokens.index('[]')\n",
    "            return tokens, target_pos\n",
    "\n",
    "    ''' \n",
    "    norm_weight\n",
    "    '''\n",
    "    # evaluation mode \n",
    "    model.eval()\n",
    "    \n",
    "    model.norm_embedding_weight(model.criterion.W)\n",
    "\n",
    "    tokens, target_pos = return_split_sentence(sentence)\n",
    "    tokens[target_pos] = unk_token\n",
    "    tokens = [bos_token] + tokens + [eos_token]\n",
    "    indexed_sentence = [stoi[token] if token in stoi else stoi[unk_token] for token in tokens]\n",
    "    input_tokens = torch.tensor(indexed_sentence, dtype=torch.long, device=device).unsqueeze(0)\n",
    "    topv, topi = model.run_inference(input_tokens, target=None, target_pos=target_pos)\n",
    "    output = []  \n",
    "    for value, key in zip(topv, topi):\n",
    "        output.append((value.item(), itos[key.item()]))\n",
    "        print(value.item(), itos[key.item()])\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8051932454109192 into\n",
      "0.5680792927742004 above\n",
      "0.25667303800582886 through\n",
      "0.0 <PAD>\n",
      "0.0 <EOS>\n",
      "0.0 <BOS>\n",
      "-0.0033715590834617615 against\n",
      "-0.02418931946158409 across\n",
      "-0.09841619431972504 alongside\n",
      "-0.25158366560935974 from\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0.8051932454109192, 'into'),\n",
       " (0.5680792927742004, 'above'),\n",
       " (0.25667303800582886, 'through'),\n",
       " (0.0, '<PAD>'),\n",
       " (0.0, '<EOS>'),\n",
       " (0.0, '<BOS>'),\n",
       " (-0.0033715590834617615, 'against'),\n",
       " (-0.02418931946158409, 'across'),\n",
       " (-0.09841619431972504, 'alongside'),\n",
       " (-0.25158366560935974, 'from')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "left_text = 'the dog ran'\n",
    "right_text = 'the house'\n",
    "\n",
    "sentence = left_text + ' [] ' + right_text\n",
    "\n",
    "run_inference_by_user_input(sentence, model, itos, stoi, unk_token=unk_token, \n",
    "                            bos_token=bos_token, eos_token=eos_token, device=device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mattevanoff/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/Users/mattevanoff/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "/Users/mattevanoff/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/mattevanoff/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:87: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with distribution=normal is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "`normal` is a deprecated alias for `truncated_normal`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.engine.training.Model at 0x1401d7470>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "sess = tf.Session()\n",
    "K.set_session(sess)\n",
    "\n",
    "ocr_model_path = 'models/ocr/models/ocr_5_len.h5'\n",
    "\n",
    "\n",
    "\n",
    "def ctc_lambda_func(args):\n",
    "    y_pred, labels, input_length, label_length = args\n",
    "    y_pred = y_pred[:, 2:, :]\n",
    "    return K.ctc_batch_cost(labels, y_pred, input_length, label_length)\n",
    "\n",
    "\n",
    "# model = load_model(ocr_model_path, custom_objects={'ctc': ctc})\n",
    "model = load_model(ocr_model_path, custom_objects={'<lambda>': lambda y_true, y_pred: y_pred})\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=L size=208x107 at 0x12831E320>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image.open(img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 128)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Roght']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_width = 128\n",
    "img_height = 64\n",
    "img_path = '../../data/raw/word_level/c03/c03-096f/c03-096f-03-05.png'\n",
    "# img_path = '../../data/raw/word_level/c03/c03-096f/c03-096f-00-05.png'\n",
    "# img_path = '../../data/raw/word_level/c03/c03-096f/c03-096f-01-00.png'\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "import itertools\n",
    "\n",
    "letters = [' ', '!', '\"', '#', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
    "def decode_batch(out):\n",
    "    ret = []\n",
    "    for j in range(out.shape[0]):\n",
    "        out_best = list(np.argmax(out[j, 2:], 1))\n",
    "        out_best = [k for k, g in itertools.groupby(out_best)]\n",
    "        outstr = ''\n",
    "        for c in out_best:\n",
    "            if c < len(letters):\n",
    "                outstr += letters[c]\n",
    "        ret.append(outstr)\n",
    "    return ret\n",
    "\n",
    "img = cv2.imread(img_path)\n",
    "# grayscale image\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "# resize image\n",
    "img = cv2.resize(img, (img_width, img_height))\n",
    "# change image type\n",
    "img = img.astype(np.float32)\n",
    "# scale image \n",
    "img /= 255\n",
    "img = img.reshape((1, img_width, img_height, 1))\n",
    "\n",
    "net_inp = model.get_layer(name='the_input').input\n",
    "net_out = model.get_layer(name='softmax').output\n",
    "\n",
    "X_data = img\n",
    "net_out_value = sess.run(net_out, feed_dict={net_inp: X_data})\n",
    "pred_texts = decode_batch(net_out_value)\n",
    "\n",
    "pred_texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LM model imports \n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "from torch import optim\n",
    "from models.context2vec.src.eval.mscc import mscc_evaluation\n",
    "from models.context2vec.src.core.nets import Context2vec\n",
    "from models.context2vec.src.util.args import parse_args\n",
    "from models.context2vec.src.util.batch import Dataset\n",
    "from models.context2vec.src.util.config import Config\n",
    "from models.context2vec.src.util.io import write_embedding, write_config, read_config, load_vocab\n",
    "\n",
    "\n",
    "# OCR model imports \n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "import cv2\n",
    "import itertools\n",
    "from models.ocr.src.config import letters\n",
    "\n",
    "from collections import defaultdict\n",
    "from operator import itemgetter\n",
    "\n",
    "\n",
    "\n",
    "class Inference():\n",
    "    def __init__(self, modelfile=None, wordsfile=None, img_width=128, img_height=64, device='cpu'):\n",
    "        \n",
    "        self.device = device\n",
    "        self.img_width = img_width\n",
    "        self.img_height = img_height\n",
    "        self.build_language_model()\n",
    "        self.build_ocr_model()\n",
    "        \n",
    "    def build_language_model(self):\n",
    "        # LANGUAGE MODEL\n",
    "        modelfile = 'models/context2vec/models/model.param'\n",
    "        wordsfile = 'models/context2vec/models/embedding.vec'\n",
    "        config_file = modelfile+'.config.json'\n",
    "        config_dict = read_config(config_file)\n",
    "        self.lm_model = Context2vec(vocab_size=config_dict['vocab_size'],\n",
    "                            counter=[1]*config_dict['vocab_size'],\n",
    "                            word_embed_size=config_dict['word_embed_size'],\n",
    "                            hidden_size=config_dict['hidden_size'],\n",
    "                            n_layers=config_dict['n_layers'],\n",
    "                            bidirectional=config_dict['bidirectional'],\n",
    "                            use_mlp=config_dict['use_mlp'],\n",
    "                            dropout=config_dict['dropout'],\n",
    "                            pad_index=config_dict['pad_index'],\n",
    "                            device=self.device,\n",
    "                            inference=True).to(self.device)\n",
    "        self.lm_model.load_state_dict(torch.load(modelfile, map_location=self.device))\n",
    "#         optimizer = optim.Adam(model.parameters(), lr=config_dict['learning_rate'])\n",
    "        # optimizer.load_state_dict(torch.load(modelfile+'.optim'))\n",
    "        self.itos, self.stoi = load_vocab(wordsfile)\n",
    "        self.unk_token = config_dict['unk_token']\n",
    "        self.bos_token = config_dict['bos_token']\n",
    "        self.eos_token = config_dict['eos_token']\n",
    "\n",
    "        \n",
    "    def build_ocr_model(self):\n",
    "        self.sess = tf.Session()\n",
    "        K.set_session(self.sess)\n",
    "\n",
    "        ocr_model_path = 'models/ocr/models/ocr_5_len.h5'\n",
    "        self.ocr_model = load_model(ocr_model_path, custom_objects={'<lambda>': lambda y_true, y_pred: y_pred})\n",
    "        \n",
    "    def preprocess_image(self, img_path, img_width, img_height):\n",
    "        img = cv2.imread(img_path)\n",
    "        # grayscale image\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        # resize image\n",
    "        img = cv2.resize(img, (img_width, img_height))\n",
    "        # change image type\n",
    "        img = img.astype(np.float32)\n",
    "        # scale image \n",
    "        img /= 255\n",
    "        img = img.reshape((1, img_width, img_height, 1))\n",
    "        return img\n",
    "        \n",
    "        \n",
    "    def _decode_batch(self, out):\n",
    "        ret = []\n",
    "        for j in range(out.shape[0]):\n",
    "            out_best = list(np.argmax(out[j, 2:], 1))\n",
    "            out_best = [k for k, g in itertools.groupby(out_best)]\n",
    "            outstr = ''\n",
    "            for c in out_best:\n",
    "                if c < len(letters):\n",
    "                    outstr += letters[c]\n",
    "            ret.append(outstr)\n",
    "        return ret\n",
    "\n",
    "        \n",
    "    def _return_split_sentence(self, sentence):\n",
    "        if ' ' not in sentence:\n",
    "            print('sentence should contain white space to split it into tokens')\n",
    "            raise SyntaxError\n",
    "        elif '[]' not in sentence:\n",
    "            print('sentence should contain `[]` that notes the target')\n",
    "            raise SyntaxError\n",
    "        else:\n",
    "            tokens = sentence.lower().strip().split()\n",
    "            target_pos = tokens.index('[]')\n",
    "            return tokens, target_pos\n",
    "        \n",
    "    def run_lm_inference_by_user_input(self, sentence, topK=30):\n",
    "\n",
    "        # evaluation mode \n",
    "        self.lm_model.eval()\n",
    "        # norm_weight\n",
    "        self.lm_model.norm_embedding_weight(self.lm_model.criterion.W)\n",
    "\n",
    "        tokens, target_pos = self._return_split_sentence(sentence)\n",
    "        tokens[target_pos] = self.unk_token\n",
    "        tokens = [self.bos_token] + tokens + [self.eos_token]\n",
    "        indexed_sentence = [self.stoi[token] if token in self.stoi else self.stoi[unk_token] for token in tokens]\n",
    "        input_tokens = \\\n",
    "            torch.tensor(indexed_sentence, dtype=torch.long, device=self.device).unsqueeze(0)\n",
    "        topv, topi = self.lm_model.run_inference(input_tokens, target=None, target_pos=target_pos, k=topK)\n",
    "        output = []  \n",
    "        for value, key in zip(topv, topi):\n",
    "            output.append((value.item(), self.itos[key.item()]))\n",
    "#             print(value.item(), self.itos[key.item()])\n",
    "        return output\n",
    "    \n",
    "    def run_ocr_inference_by_user_image(self, img):\n",
    "        net_inp = self.ocr_model.get_layer(name='the_input').input\n",
    "        net_out = self.ocr_model.get_layer(name='softmax').output\n",
    "        net_out_value = self.sess.run(net_out, feed_dict={net_inp: img})\n",
    "        pred_texts = self._decode_batch(net_out_value)\n",
    "        return pred_texts\n",
    "    \n",
    "    from collections import defaultdict\n",
    "    from operator import itemgetter\n",
    "\n",
    "\n",
    "    def create_features(self, lm_preds, ocr_pred):\n",
    "\n",
    "        # not used currently\n",
    "        # ----------------------------------\n",
    "        bins = {\n",
    "            'small': list(range(0, 3)),\n",
    "            'small-mid': list(range(2, 6)),\n",
    "            'mid': list(range(4, 8)),\n",
    "            'mid-large': list(range(6, 10)),\n",
    "            'large': list(range(8, 12)),\n",
    "            'large-big': list(range(10, 14)),\n",
    "            'big': list(range(12, 100)),\n",
    "        }\n",
    "\n",
    "        bins = defaultdict(lambda: 'na', bins)\n",
    "\n",
    "        ocr_len = len([x for x in ocr_pred[0]])\n",
    "        pred_bins = [k for k, v in bins.items() if ocr_len in v]\n",
    "        # ----------------------------------\n",
    "\n",
    "        features = {}\n",
    "        bad_list = ['<PAD>', '<BOS>', '<EOS>', '<UNK>'] # ADD foul words\n",
    "        matches_non_ordered = {}\n",
    "        matches = {}\n",
    "\n",
    "        ocr_pred_lower = ocr_pred[0].lower()\n",
    "\n",
    "        for lm_pred in lm_preds:\n",
    "            score, word = lm_pred[0], lm_pred[1].rstrip()\n",
    "            word = word.lower()\n",
    "            # remove pad, bos, etc...\n",
    "            if word not in bad_list:\n",
    "\n",
    "                features[word] = {}\n",
    "                features[word]['score'] = score\n",
    "                # match first and last character \n",
    "                first_char_match = word[0] == ocr_pred_lower[0]\n",
    "                last_char_match = word[-1] == ocr_pred_lower[-1]\n",
    "                features[word]['first_char_match'] = first_char_match\n",
    "                features[word]['last_char_match'] = last_char_match\n",
    "\n",
    "                num_chars = 0\n",
    "                for char in ocr_pred_lower:\n",
    "                    if char in word:\n",
    "                        num_chars += 1\n",
    "                    matches[word] = num_chars\n",
    "                features[word]['num_matches'] = matches[word]  \n",
    "\n",
    "        return features\n",
    "\n",
    "\n",
    "    def final_scores(self, features):\n",
    "        final_scores = {}\n",
    "\n",
    "        for word, feature_dict in features.items():\n",
    "            final_score = 1\n",
    "            first_char_match = feature_dict['first_char_match']\n",
    "            last_char_match = feature_dict['last_char_match']\n",
    "            score = feature_dict['score']\n",
    "            if first_char_match:\n",
    "                final_score += 10\n",
    "            if last_char_match:\n",
    "                final_score += 10\n",
    "            final_score *= score\n",
    "            final_scores[word] = final_score\n",
    "        top_results = sorted(final_scores.items(), key=itemgetter(1), reverse=True)\n",
    "        return top_results[0][0]\n",
    "\n",
    "    def weigh_function(self):\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def predict(self, sentence, img_path):\n",
    "        lm_preds = self.run_lm_inference_by_user_input(sentence)\n",
    "        \n",
    "        img = self.preprocess_image(img_path, self.img_width, self.img_height)\n",
    "        ocr_pred = self.run_ocr_inference_by_user_image(img)\n",
    "\n",
    "        features = self.create_features(lm_preds, ocr_pred)\n",
    "        final_pred = self.final_scores(features)\n",
    "    \n",
    "        # return top K? And use MAP @ K ??\n",
    "        return final_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_text = 'the dog ran'\n",
    "right_text = 'the house'\n",
    "\n",
    "sentence = left_text + ' [] ' + right_text\n",
    "\n",
    "img_width = 128\n",
    "img_height = 64\n",
    "img_path = '../../data/raw/word_level/c03/c03-096f/c03-096f-03-05.png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference = Inference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'into'"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference.predict(sentence, img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = {\n",
    "    list(range(0, 3)): 'small',\n",
    "    list(range(2, 5)): 'small-mid',\n",
    "    list(range(4, 7)): 'mid',\n",
    "    list(range(6, 9)): 'mid-large',\n",
    "    list(range(8, 11)): 'large',\n",
    "    list(range(10, 13)): 'large-big',\n",
    "    list(range(12, 100)): 'big',\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "bins = {\n",
    "    'small': list(range(0, 3)),\n",
    "    'small-mid': list(range(2, 6)),\n",
    "    'mid': list(range(4, 8)),\n",
    "    'mid-large': list(range(6, 10)),\n",
    "    'large': list(range(8, 12)),\n",
    "    'large-big': list(range(10, 14)),\n",
    "    'big': list(range(12, 100)),\n",
    "}\n",
    "\n",
    "bins = defaultdict(lambda: 'na', bins)\n",
    "\n",
    "ocr_len = len([x for x in ocr_pred[0]])\n",
    "pred_bins = [k for k, v in bins.items() if ocr_len in v]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['small-mid', 'mid']"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'throughout': 4}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matches = {}\n",
    "\n",
    "word = 'throughout'\n",
    "num_chars = 0\n",
    "for char in ocr_pred[0]:\n",
    "    if char in word:\n",
    "        num_chars += 1\n",
    "    matches[word] = num_chars\n",
    "    \n",
    "matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "features = {}\n",
    "bad_list = ['<PAD>', '<BOS>', '<EOS>', '<UNK>'] # ADD foul words\n",
    "matches_non_ordered = {}\n",
    "\n",
    "ocr_pred_lower = ocr_pred[0].lower()\n",
    "\n",
    "for lm_pred in lm_preds:\n",
    "    score, word = lm_pred[0], lm_pred[1].rstrip()\n",
    "    word = word.lower()\n",
    "    # remove pad, bos, etc...\n",
    "    if word not in bad_list:\n",
    "        \n",
    "        features[word] = {}\n",
    "        features[word]['score'] = score\n",
    "        # match first and last character \n",
    "        first_char_match = word[0] == ocr_pred_lower[0]\n",
    "        last_char_match = word[-1] == ocr_pred_lower[-1]\n",
    "        features[word]['first_char_match'] = first_char_match\n",
    "        features[word]['last_char_match'] = last_char_match\n",
    "\n",
    "        num_chars = 0\n",
    "        for char in ocr_pred_lower:\n",
    "            if char in word:\n",
    "                num_chars += 1\n",
    "            matches[word] = num_chars\n",
    "        features[word]['num_matches'] = matches[word]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'into'"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "final_scores = {}\n",
    "\n",
    "for word, feature_dict in features.items():\n",
    "    final_score = 1\n",
    "    first_char_match = feature_dict['first_char_match']\n",
    "    last_char_match = feature_dict['last_char_match']\n",
    "    score = feature_dict['score']\n",
    "    if first_char_match:\n",
    "        final_score += 10\n",
    "    if last_char_match:\n",
    "        final_score += 10\n",
    "    final_score *= score\n",
    "    final_scores[word] = final_score\n",
    "top_results = sorted(final_scores.items(), key=itemgetter(1), reverse=True)\n",
    "top_results[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from operator import itemgetter\n",
    "\n",
    "\n",
    "def create_features(lm_preds, ocr_pred):\n",
    "\n",
    "    # not used currently\n",
    "    # ----------------------------------\n",
    "    bins = {\n",
    "        'small': list(range(0, 3)),\n",
    "        'small-mid': list(range(2, 6)),\n",
    "        'mid': list(range(4, 8)),\n",
    "        'mid-large': list(range(6, 10)),\n",
    "        'large': list(range(8, 12)),\n",
    "        'large-big': list(range(10, 14)),\n",
    "        'big': list(range(12, 100)),\n",
    "    }\n",
    "\n",
    "    bins = defaultdict(lambda: 'na', bins)\n",
    "\n",
    "    ocr_len = len([x for x in ocr_pred[0]])\n",
    "    pred_bins = [k for k, v in bins.items() if ocr_len in v]\n",
    "    # ----------------------------------\n",
    "\n",
    "    features = {}\n",
    "    bad_list = ['<PAD>', '<BOS>', '<EOS>', '<UNK>'] # ADD foul words\n",
    "    matches_non_ordered = {}\n",
    "\n",
    "    ocr_pred_lower = ocr_pred[0].lower()\n",
    "\n",
    "    for lm_pred in lm_preds:\n",
    "        score, word = lm_pred[0], lm_pred[1].rstrip()\n",
    "        word = word.lower()\n",
    "        # remove pad, bos, etc...\n",
    "        if word not in bad_list:\n",
    "\n",
    "            features[word] = {}\n",
    "            features[word]['score'] = score\n",
    "            # match first and last character \n",
    "            first_char_match = word[0] == ocr_pred_lower[0]\n",
    "            last_char_match = word[-1] == ocr_pred_lower[-1]\n",
    "            features[word]['first_char_match'] = first_char_match\n",
    "            features[word]['last_char_match'] = last_char_match\n",
    "\n",
    "            num_chars = 0\n",
    "            for char in ocr_pred_lower:\n",
    "                if char in word:\n",
    "                    num_chars += 1\n",
    "                matches[word] = num_chars\n",
    "            features[word]['num_matches'] = matches[word]  \n",
    "            \n",
    "    return features\n",
    "\n",
    "\n",
    "def final_scores(features):\n",
    "    final_scores = {}\n",
    "\n",
    "    for word, feature_dict in features.items():\n",
    "        final_score = 1\n",
    "        first_char_match = feature_dict['first_char_match']\n",
    "        last_char_match = feature_dict['last_char_match']\n",
    "        score = feature_dict['score']\n",
    "        if first_char_match:\n",
    "            final_score += 10\n",
    "        if last_char_match:\n",
    "            final_score += 10\n",
    "        final_score *= score\n",
    "        final_scores[word] = final_score\n",
    "    top_results = sorted(final_scores.items(), key=itemgetter(1), reverse=True)\n",
    "    return top_results[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'into'"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_preds, ocr_pred = inference.predict(sentence, img_path)\n",
    "\n",
    "features = create_features(lm_preds, ocr_pred)\n",
    "final_scores(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=L size=134x70 at 0x7FB5F42499E8>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "img_path = '../../data/raw/word_level/c03/c03-096f/c03-096f-07-05.png'\n",
    "Image.open(img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matt/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/OCRBeamSearch/model/snapshot-38\n",
      "INFO:tensorflow:Restoring parameters from models/OCRBeamSearch/model/snapshot-38\n"
     ]
    }
   ],
   "source": [
    "from inference import Inference\n",
    "\n",
    "inference_model = Inference(img_width=128, img_height=64, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('live', (0.6397737264633179, 'slept'), ['like'], array([0.82690567], dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "file_url = '../../data/raw/word_level/c03/c03-096f/c03-096f-00-07.png'\n",
    "\n",
    "file_url = '../../data/raw/word_level/c03/c03-096f/c03-096f-07-05.png'\n",
    "left_text = 'We'\n",
    "right_text = 'in the house'\n",
    "X = left_text + ' [] ' + right_text\n",
    "prediction = inference_model.predict(X, file_url, ind_preds=True, ocr_prob_threshold=0.85)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = [(0.6397737264633179, 'slept'), (0.6323035359382629, 'dabble'), (0.559866189956665, \"'re\"), (0.5329194068908691, 'stayed'), (0.5288676023483276, 'sat'), (0.4934336543083191, 'lived'), (0.4536677598953247, 'hid'), (0.4443860352039337, 'got'), (0.43601545691490173, 'live'), (0.4348217248916626, 'buried'), (0.41707414388656616, 'rejoice'), (0.3848378360271454, 'vacationed'), (0.3815666437149048, 'congregated'), (0.3779458701610565, 'balked'), (0.35714060068130493, 'dove'), (0.35361653566360474, 'crouched'), (0.3484594523906708, 'lunched'), (0.3433127999305725, 'toiled'), (0.3385561406612396, 'huddled'), (0.3336961269378662, 'prospered'), (0.3314686715602875, 'dwelled'), (0.3291795551776886, 'were'), (0.32495132088661194, 'strolled'), (0.32009977102279663, 'resided'), (0.319406658411026, 'reside'), (0.30158546566963196, 'landed'), (0.2980598509311676, 'squatted'), (0.2967756390571594, 'partook'), (0.29490840435028076, 'kept'), (0.28834477066993713, 'labored'), (0.2835155427455902, 'belong'), (0.27830618619918823, 'sit'), (0.27349746227264404, 'rehearsed'), (0.2687121629714966, 'lie'), (0.26786699891090393, 'remained'), (0.2676231265068054, 'dined'), (0.26738911867141724, 'holidayed'), (0.266904354095459, 'waited'), (0.2668074667453766, 'appear'), (0.2650832235813141, 'exist'), (0.2648472189903259, 'arrive'), (0.26464134454727173, 'dwelt'), (0.2621450424194336, 'remain'), (0.256123423576355, 'rehearse'), (0.2519649565219879, 'are'), (0.2515889108181, 'conversed'), (0.25145041942596436, 'settled'), (0.25047653913497925, 'saw'), (0.2497512698173523, 'gathered'), (0.24547839164733887, 'lodged'), (0.24530239403247833, 'lurk'), (0.23731063306331635, 'gaped'), (0.23484927415847778, 'congregate'), (0.23070558905601501, 'tunneled'), (0.22245654463768005, 'stepped'), (0.22142094373703003, 'stay'), (0.22085174918174744, 'locked'), (0.2196076363325119, 'overindulge'), (0.2187119424343109, 'specialise'), (0.21519632637500763, 'suffocated'), (0.21490679681301117, 'kneel'), (0.21052272617816925, 'swore'), (0.20960599184036255, 'reveled'), (0.20607803761959076, 'prayed'), (0.20335553586483002, 'dabbled'), (0.20228329300880432, 'camped'), (0.19979387521743774, 'reeled'), (0.19743305444717407, 'put'), (0.1950840801000595, 'died'), (0.19413556158542633, 'invested'), (0.19234266877174377, 'sang'), (0.18997767567634583, 'walked'), (0.18307025730609894, 'swam'), (0.18282797932624817, 'worked'), (0.18175747990608215, 'ate'), (0.1817299872636795, 'appeared'), (0.17974412441253662, 'stare'), (0.17921455204486847, 'met'), (0.17631219327449799, 'boxed'), (0.17625883221626282, 'hibernated'), (0.17165663838386536, 'languish'), (0.16872477531433105, 'wallow'), (0.1677539348602295, 'dwell'), (0.16713020205497742, 'alighted'), (0.16612690687179565, 'figured'), (0.16421453654766083, 'laughed'), (0.16245490312576294, 'rejoiced'), (0.16179195046424866, 'reappear'), (0.1598750352859497, 'stood'), (0.1593179702758789, 'see'), (0.15727050602436066, 'Remain'), (0.15393124520778656, 'speak'), (0.15130959451198578, 'marched'), (0.15105397999286652, 'stopped'), (0.15047335624694824, 'swung'), (0.14485952258110046, 'huddle'), (0.14388787746429443, 'awake'), (0.14376991987228394, 'gather'), (0.14287489652633667, 'clapped')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_url = '../../data/raw/word_level/c03/c03-096f/c03-096f-07-05.png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We [] in the house\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=L size=134x70 at 0x7EFD11357978>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " X = 'We' + ' [] ' + 'in the house'\n",
    "print(X)\n",
    "from PIL import Image\n",
    "Image.open(file_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OCR prediction is \"like\" with probability of 83.0%\n"
     ]
    }
   ],
   "source": [
    "ocr_pred, ocr_prob = inference_model.run_beam_ocr_inference_by_user_image(file_url)\n",
    "print('OCR prediction is \"{}\" with probability of {}%'.format(ocr_pred[0], round(ocr_prob[0]*100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.6397737264633179, 'slept'), (0.6323035359382629, 'dabble'), (0.559866189956665, \"'re\"), (0.5329194068908691, 'stayed'), (0.5288676023483276, 'sat'), (0.4934336543083191, 'lived'), (0.4536677598953247, 'hid'), (0.4443860352039337, 'got'), (0.43601545691490173, 'live')]\n",
      "Top 10 LM predictions: ['slept', 'dabble', \"'re\", 'stayed', 'sat', 'lived', 'hid', 'got', 'live']\n"
     ]
    }
   ],
   "source": [
    "lm_preds = inference_model.run_lm_inference_by_user_input(X, topK=10)\n",
    "print(lm_preds)\n",
    "print('Top 10 LM predictions: {}'.format([w for _, w in lm_preds]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('live', 4.053264933659698),\n",
       " ('lived', 3.0220344079222583),\n",
       " ('dabble', 2.4400122671680546),\n",
       " (\"'re\", 2.3418169876984027),\n",
       " ('slept', 2.1773005375385295),\n",
       " ('hid', 1.8956185576370623),\n",
       " ('stayed', 1.815633970481934),\n",
       " ('sat', 1.649077876730878),\n",
       " ('got', 1.564596309586484)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = inference_model.create_features_improved(lm_preds, ocr_pred, ocr_prob)\n",
    "inference_model.final_scores(features, ocr_pred, ocr_prob_threshold=0.8, return_topK=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('live', 4.8623097696683555),\n",
       " ('lived', 3.448472232239753),\n",
       " ('dabble', 3.00382016921238),\n",
       " (\"'re\", 2.888073804708552),\n",
       " ('slept', 2.5013190095196265),\n",
       " ('hid', 2.161875374647212),\n",
       " ('stayed', 1.9861207593784505),\n",
       " ('sat', 1.7082426527844938),\n",
       " ('got', 1.6237610856401)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = inference_model.create_features_improved(lm_preds, ocr_pred, ocr_prob)\n",
    "inference_model.final_scores(features, ocr_pred, ocr_prob_threshold=0.85, return_topK=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('live', 2.673293292529275),\n",
       " ('lie', 2.3923764273104235),\n",
       " ('lived', 2.2756261636663577),\n",
       " ('lurk', 2.08051279040776),\n",
       " ('locked', 1.9274139822916463),\n",
       " ('lunched', 1.8856334088129076),\n",
       " ('landed', 1.8714337629911693),\n",
       " ('dabble', 1.8587313879662044),\n",
       " ('labored', 1.825518727092174),\n",
       " ('lodged', 1.8153266889688762)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "features = inference_model.create_features_improved(Z, ['like'], [0.7777384])\n",
    "inference_model.final_scores(features, ['like'], 0.8)\n",
    "weights = inference_model.get_weights()\n",
    "    \n",
    "final_scores = {}\n",
    "for word, dic in features.items():\n",
    "    for feature in weights.keys():\n",
    "        features[word].update({feature: (features[word][feature] * weights[feature])})\n",
    "    final_scores[word] = sum(features[word].values())\n",
    "    \n",
    "    \n",
    "top_results = sorted(final_scores.items(), key=itemgetter(1), reverse=True)\n",
    "top_results[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "slept\n",
      "dabble\n",
      "'re\n",
      "stayed\n",
      "sat\n",
      "lived\n",
      "hid\n",
      "got\n",
      "live\n",
      "buried\n",
      "rejoice\n",
      "vacationed\n",
      "congregated\n",
      "balked\n",
      "dove\n",
      "crouched\n",
      "lunched\n",
      "toiled\n",
      "huddled\n",
      "prospered\n",
      "dwelled\n",
      "were\n",
      "strolled\n",
      "resided\n",
      "reside\n",
      "landed\n",
      "squatted\n",
      "partook\n",
      "kept\n",
      "labored\n",
      "belong\n",
      "sit\n",
      "rehearsed\n",
      "lie\n",
      "remained\n",
      "dined\n",
      "holidayed\n",
      "waited\n",
      "appear\n",
      "exist\n",
      "arrive\n",
      "dwelt\n",
      "remain\n",
      "rehearse\n",
      "are\n",
      "conversed\n",
      "settled\n",
      "saw\n",
      "gathered\n",
      "lodged\n",
      "lurk\n",
      "gaped\n",
      "congregate\n",
      "tunneled\n",
      "stepped\n",
      "stay\n",
      "locked\n",
      "overindulge\n",
      "specialise\n",
      "suffocated\n",
      "kneel\n",
      "swore\n",
      "reveled\n",
      "prayed\n",
      "dabbled\n",
      "camped\n",
      "reeled\n",
      "put\n",
      "died\n",
      "invested\n",
      "sang\n",
      "walked\n",
      "swam\n",
      "worked\n",
      "ate\n",
      "appeared\n",
      "stare\n",
      "met\n",
      "boxed\n",
      "hibernated\n",
      "languish\n",
      "wallow\n",
      "dwell\n",
      "alighted\n",
      "figured\n",
      "laughed\n",
      "rejoiced\n",
      "reappear\n",
      "stood\n",
      "see\n",
      "Remain\n",
      "speak\n",
      "marched\n",
      "stopped\n",
      "swung\n",
      "huddle\n",
      "awake\n",
      "gather\n",
      "clapped\n"
     ]
    }
   ],
   "source": [
    "for prob, word in Z:\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "the_string = '' + ' [] '  +  ''\n",
    "# the_string = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "if re.search('[a-zA-Z]', the_string) is not None:\n",
    "    print(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "if not os.path.isfile('.'):\n",
    "    print(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(1.224109172821045, '<UNK>'), (0.4631592333316803, 'mutual'), (0.45344918966293335, 'filial'), (0.4316594898700714, 'chronological'), (0.43153640627861023, 'direct'), (0.425195574760437, 'regal'), (0.41831493377685547, 'slavish'), (0.41372278332710266, 'relative'), (0.38065898418426514, 'divine'), (0.37932538986206055, 'comparative')]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
