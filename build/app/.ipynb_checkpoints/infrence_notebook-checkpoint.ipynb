{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "into\n"
     ]
    }
   ],
   "source": [
    "# LM model imports \n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "from torch import optim\n",
    "from models.context2vec.src.eval.mscc import mscc_evaluation\n",
    "from models.context2vec.src.core.nets import Context2vec\n",
    "from models.context2vec.src.util.args import parse_args\n",
    "from models.context2vec.src.util.batch import Dataset\n",
    "from models.context2vec.src.util.config import Config\n",
    "from models.context2vec.src.util.io import write_embedding, write_config, read_config, load_vocab\n",
    "\n",
    "\n",
    "# OCR model imports \n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "import cv2\n",
    "import itertools\n",
    "from models.ocr.src.config import letters\n",
    "\n",
    "\n",
    "# TODO \n",
    "# -- need to add if the model is not loaded -- error print statement \n",
    "\n",
    "\n",
    "# LM model imports \n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "from torch import optim\n",
    "from models.context2vec.src.eval.mscc import mscc_evaluation\n",
    "from models.context2vec.src.core.nets import Context2vec\n",
    "from models.context2vec.src.util.args import parse_args\n",
    "from models.context2vec.src.util.batch import Dataset\n",
    "from models.context2vec.src.util.config import Config\n",
    "from models.context2vec.src.util.io import write_embedding, write_config, read_config, load_vocab\n",
    "\n",
    "\n",
    "# OCR model imports \n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "import cv2\n",
    "import itertools\n",
    "from models.ocr.src.config import letters\n",
    "\n",
    "from collections import defaultdict\n",
    "from operator import itemgetter\n",
    "\n",
    "\n",
    "\n",
    "class Inference():\n",
    "    def __init__(self, modelfile=None, wordsfile=None, img_width=128, img_height=64, device='cpu'):\n",
    "        \n",
    "        self.device = device\n",
    "        self.img_width = img_width\n",
    "        self.img_height = img_height\n",
    "        self.build_language_model()\n",
    "        self.build_ocr_model()\n",
    "        \n",
    "    def build_language_model(self):\n",
    "        # LANGUAGE MODEL\n",
    "        modelfile = 'models/context2vec/models/model.param'\n",
    "        wordsfile = 'models/context2vec/models/embedding.vec'\n",
    "        config_file = modelfile+'.config.json'\n",
    "        config_dict = read_config(config_file)\n",
    "        self.lm_model = Context2vec(vocab_size=config_dict['vocab_size'],\n",
    "                            counter=[1]*config_dict['vocab_size'],\n",
    "                            word_embed_size=config_dict['word_embed_size'],\n",
    "                            hidden_size=config_dict['hidden_size'],\n",
    "                            n_layers=config_dict['n_layers'],\n",
    "                            bidirectional=config_dict['bidirectional'],\n",
    "                            use_mlp=config_dict['use_mlp'],\n",
    "                            dropout=config_dict['dropout'],\n",
    "                            pad_index=config_dict['pad_index'],\n",
    "                            device=self.device,\n",
    "                            inference=True).to(self.device)\n",
    "        self.lm_model.load_state_dict(torch.load(modelfile, map_location=self.device))\n",
    "#         optimizer = optim.Adam(model.parameters(), lr=config_dict['learning_rate'])\n",
    "        # optimizer.load_state_dict(torch.load(modelfile+'.optim'))\n",
    "        self.itos, self.stoi = load_vocab(wordsfile)\n",
    "        self.unk_token = config_dict['unk_token']\n",
    "        self.bos_token = config_dict['bos_token']\n",
    "        self.eos_token = config_dict['eos_token']\n",
    "\n",
    "        \n",
    "    def build_ocr_model(self):\n",
    "        self.sess = tf.Session()\n",
    "        K.set_session(self.sess)\n",
    "\n",
    "        ocr_model_path = 'models/ocr/models/weights-improvement2-10-01-3.00.hdf5'\n",
    "        self.ocr_model = load_model(ocr_model_path, custom_objects={'<lambda>': lambda y_true, y_pred: y_pred})\n",
    "        \n",
    "    def preprocess_image(self, img_path, img_width, img_height):\n",
    "        img = cv2.imread(img_path)\n",
    "        # grayscale image\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        # resize image\n",
    "        img = cv2.resize(img, (img_width, img_height))\n",
    "        # change image type\n",
    "        img = img.astype(np.float32)\n",
    "        # scale image \n",
    "        img /= 255\n",
    "        img = img.reshape((1, img_width, img_height, 1))\n",
    "        return img\n",
    "        \n",
    "        \n",
    "    def _decode_batch(self, out):\n",
    "        ret = []\n",
    "        for j in range(out.shape[0]):\n",
    "            out_best = list(np.argmax(out[j, 2:], 1))\n",
    "            out_best = [k for k, g in itertools.groupby(out_best)]\n",
    "            outstr = ''\n",
    "            for c in out_best:\n",
    "                if c < len(letters):\n",
    "                    outstr += letters[c]\n",
    "            ret.append(outstr)\n",
    "        return ret\n",
    "\n",
    "        \n",
    "    def _return_split_sentence(self, sentence):\n",
    "        if ' ' not in sentence:\n",
    "            print('sentence should contain white space to split it into tokens')\n",
    "            raise SyntaxError\n",
    "        elif '[]' not in sentence:\n",
    "            print('sentence should contain `[]` that notes the target')\n",
    "            raise SyntaxError\n",
    "        else:\n",
    "            tokens = sentence.lower().strip().split()\n",
    "            target_pos = tokens.index('[]')\n",
    "            return tokens, target_pos\n",
    "        \n",
    "    def run_lm_inference_by_user_input(self, sentence, topK=30):\n",
    "\n",
    "        # evaluation mode \n",
    "        self.lm_model.eval()\n",
    "        # norm_weight\n",
    "        self.lm_model.norm_embedding_weight(self.lm_model.criterion.W)\n",
    "\n",
    "        tokens, target_pos = self._return_split_sentence(sentence)\n",
    "        tokens[target_pos] = self.unk_token\n",
    "        tokens = [self.bos_token] + tokens + [self.eos_token]\n",
    "        indexed_sentence = [self.stoi[token] if token in self.stoi else self.stoi[self.unk_token] for token in tokens]\n",
    "        input_tokens = \\\n",
    "            torch.tensor(indexed_sentence, dtype=torch.long, device=self.device).unsqueeze(0)\n",
    "        topv, topi = self.lm_model.run_inference(input_tokens, target=None, target_pos=target_pos, k=topK)\n",
    "        output = []  \n",
    "        for value, key in zip(topv, topi):\n",
    "            output.append((value.item(), self.itos[key.item()]))\n",
    "#             print(value.item(), self.itos[key.item()])\n",
    "        return output\n",
    "    \n",
    "    def run_ocr_inference_by_user_image(self, img):\n",
    "        net_inp = self.ocr_model.get_layer(name='the_input').input\n",
    "        net_out = self.ocr_model.get_layer(name='softmax').output\n",
    "        net_out_value = self.sess.run(net_out, feed_dict={net_inp: img})\n",
    "        pred_texts = self._decode_batch(net_out_value)\n",
    "        return pred_texts\n",
    "    \n",
    "    from collections import defaultdict\n",
    "    from operator import itemgetter\n",
    "\n",
    "\n",
    "    def create_features(self, lm_preds, ocr_pred):\n",
    "\n",
    "        # not used currently\n",
    "        # ----------------------------------\n",
    "        bins = {\n",
    "            'small': list(range(0, 3)),\n",
    "            'small-mid': list(range(2, 6)),\n",
    "            'mid': list(range(4, 8)),\n",
    "            'mid-large': list(range(6, 10)),\n",
    "            'large': list(range(8, 12)),\n",
    "            'large-big': list(range(10, 14)),\n",
    "            'big': list(range(12, 100)),\n",
    "        }\n",
    "\n",
    "        bins = defaultdict(lambda: 'na', bins)\n",
    "\n",
    "        ocr_len = len([x for x in ocr_pred[0]])\n",
    "        pred_bins = [k for k, v in bins.items() if ocr_len in v]\n",
    "        # ----------------------------------\n",
    "\n",
    "        features = {}\n",
    "        bad_list = ['<PAD>', '<BOS>', '<EOS>', '<UNK>'] # ADD foul words\n",
    "        matches = {}\n",
    "        matches_non_ordered = {}\n",
    "\n",
    "        ocr_pred_lower = ocr_pred[0].lower()\n",
    "\n",
    "        for lm_pred in lm_preds:\n",
    "            score, word = lm_pred[0], lm_pred[1].rstrip()\n",
    "            word = word.lower()\n",
    "            # remove pad, bos, etc...\n",
    "            if word not in bad_list:\n",
    "                try:\n",
    "                    features[word] = {}\n",
    "                    features[word]['score'] = score\n",
    "                    # match first and last character \n",
    "                    first_char_match = word[0] == ocr_pred_lower[0]\n",
    "                    last_char_match = word[-1] == ocr_pred_lower[-1]\n",
    "                    features[word]['first_char_match'] = first_char_match\n",
    "                    features[word]['last_char_match'] = last_char_match\n",
    "\n",
    "                    num_chars = 0\n",
    "                    for char in ocr_pred_lower:\n",
    "                        if char in word:\n",
    "                            num_chars += 1\n",
    "                        matches[word] = num_chars\n",
    "                    features[word]['num_matches'] = matches[word] \n",
    "                except:\n",
    "                    pass \n",
    "\n",
    "        return features\n",
    "\n",
    "\n",
    "    def final_scores(self, features):\n",
    "        final_scores = {}\n",
    "\n",
    "        for word, feature_dict in features.items():\n",
    "            final_score = 1\n",
    "            first_char_match = feature_dict['first_char_match']\n",
    "            last_char_match = feature_dict['last_char_match']\n",
    "            score = feature_dict['score']\n",
    "            if first_char_match:\n",
    "                final_score += 10\n",
    "            if last_char_match:\n",
    "                final_score += 10\n",
    "            final_score *= score\n",
    "            final_scores[word] = final_score\n",
    "        top_results = sorted(final_scores.items(), key=itemgetter(1), reverse=True)\n",
    "        return top_results[0][0]\n",
    "\n",
    "    def weigh_function(self):\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def predict(self, sentence, img_path):\n",
    "        lm_preds = self.run_lm_inference_by_user_input(sentence)\n",
    "        \n",
    "        img = self.preprocess_image(img_path, self.img_width, self.img_height)\n",
    "        ocr_pred = self.run_ocr_inference_by_user_image(img)\n",
    "\n",
    "        features = self.create_features(lm_preds, ocr_pred)\n",
    "        final_pred = self.final_scores(features)\n",
    "    \n",
    "        # return top K? And use MAP @ K ??\n",
    "        return final_pred\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    left_text = 'the dog ran'\n",
    "    right_text = 'the house'\n",
    "\n",
    "    sentence = left_text + ' [] ' + right_text\n",
    "\n",
    "    img_width = 128\n",
    "    img_height = 64\n",
    "    img_path = '../../data/sample/c03-096f-03-05.png'\n",
    "\n",
    "\n",
    "    inference = Inference()\n",
    "\n",
    "    print(inference.predict(sentence, img_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;35m../../data/sample/c03-096f-03-05.png\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "ls ../../data/sample/c03-096f-03-05.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install opencv-python==3.4.0.12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.4.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
