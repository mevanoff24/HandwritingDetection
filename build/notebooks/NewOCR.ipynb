{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mattevanoff/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/Users/mattevanoff/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "/Users/mattevanoff/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/Users/mattevanoff/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "import os\n",
    "from os.path import join\n",
    "import json\n",
    "import random\n",
    "import itertools\n",
    "import re\n",
    "import datetime\n",
    "import numpy as np\n",
    "from scipy import ndimage\n",
    "import pylab\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "%matplotlib inline\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.layers import Input, Dense, Activation\n",
    "from tensorflow.keras.layers import Reshape, Lambda\n",
    "from tensorflow.keras.layers import add, concatenate\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import GRU\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.utils import get_file\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import tensorflow.keras.callbacks\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "sess = tf.Session()\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example.txt         meta.json           meta_json.json\r\n",
      "meta.csv            meta_json.csv       word_level_meta.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls ../../data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique = meta.drop_duplicates(subset=['document'], keep='last')\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "all_tokens = Counter()\n",
    "all_lower_tokens = Counter()\n",
    "all_chars = Counter()\n",
    "all_lower_chars = Counter()\n",
    "all_letters = ''\n",
    "i = 1\n",
    "# for tokens in unique.meta[:10]:\n",
    "for tokens in unique.meta:\n",
    "    i += 2\n",
    "    for token in tokens:\n",
    "        all_letters += token\n",
    "        all_tokens[token] += 1\n",
    "        all_lower_tokens[token.lower()] += 1\n",
    "        for char in token:\n",
    "            all_chars[char] += 1\n",
    "            all_lower_chars[char.lower()] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import dill as pickle\n",
    "\n",
    "\n",
    "def labels_to_text(labels):\n",
    "    return ''.join(list(map(lambda x: all_letters[int(x)], labels)))\n",
    "\n",
    "def text_to_labels(text):\n",
    "    return list(map(lambda x: all_letters.index(x), text))\n",
    "\n",
    "def is_valid_str(s):\n",
    "    for ch in s:\n",
    "        if not ch in all_letters:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def unpickle(filename):\n",
    "    \"\"\"Unpickle file\"\"\"\n",
    "    with open(filename, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "class TextImageGenerator:\n",
    "    \n",
    "    def __init__(self, word_level_df, data_path, img_width, img_height, batch_size, downsample_factor, \n",
    "                 max_text_len=21+1, pre_pad=True, save_letters=True, use_s3=False):\n",
    "        \n",
    "#         self.letters = letters\n",
    "#         # create letters mapping \n",
    "#         # +2 0 for pad value and 1 for OOV\n",
    "#         self.letter2idx = defaultdict(lambda: 1, {x: i+2 for i, x in enumerate(sorted(letters))})\n",
    "#         self.letter2idx['<unk>'] = 1\n",
    "#         self.letter2idx['<pad>'] = 0\n",
    "#         self.pad_idx = self.letter2idx['<pad>']\n",
    "#         self.idx2letter = {v: k for k, v in self.letter2idx.items()}\n",
    "        \n",
    "#         if save_letters:\n",
    "#             with open(os.path.join('../../data/', 'processed', 'letters_map.pkl'), 'wb') as f:\n",
    "#                 pickle.dump(self.letter2idx, f, protocol=2)\n",
    "            \n",
    "        self.data_path = data_path\n",
    "        self.img_width = img_width\n",
    "        self.img_height = img_height\n",
    "        self.batch_size = batch_size\n",
    "        self.downsample_factor = downsample_factor\n",
    "        self.max_text_len = max_text_len\n",
    "        self.pre_pad = pre_pad\n",
    "        \n",
    "        # TODO: should I create DF as well? \n",
    "        self.word_level_df = word_level_df\n",
    "#         self.word_level_df = create_word_level_df(meta)\n",
    "#         self.word_level_df = create_image_path(self.word_level_df, data_path)\n",
    "        \n",
    "        # training data \n",
    "        self.samples = self.word_level_df[['image_path', 'token']].values.tolist()\n",
    "        self.N = len(self.samples)\n",
    "        self.current_index = 0\n",
    "        \n",
    "    def build_data(self):\n",
    "        self.images = np.zeros((self.N, self.img_height, self.img_width))\n",
    "        self.texts = []\n",
    "        bad_records = []\n",
    "        for i, (img_path, text) in enumerate(self.samples):\n",
    "            try:\n",
    "                # read image \n",
    "                img = cv2.imread(img_path)\n",
    "                # grayscale image\n",
    "                img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "                # resize image\n",
    "                img = cv2.resize(img, (self.img_width, self.img_height))\n",
    "                # change image type\n",
    "                img = img.astype(np.float32)\n",
    "                # scale image \n",
    "                img /= 255\n",
    "                # width and height are backwards from typical Keras convention\n",
    "                # because width is the time dimension when it gets fed into the RNN\n",
    "                self.images[i, :, :] = img\n",
    "                self.texts.append(text)\n",
    "            except:\n",
    "                print('Image not available for image', i, img_path, text)\n",
    "                bad_records.append(i)\n",
    "        # update stats to remove bad records with no image data \n",
    "        self.N -= len(bad_records)\n",
    "        self.indexes = list(range(self.N))\n",
    "        self.images = np.delete(self.images, bad_records, axis=0)\n",
    "\n",
    "    def get_output_size(self):\n",
    "#         return len(self.letters) + 1\n",
    "        return len(all_letters) + 1\n",
    "\n",
    "    def next_sample(self):\n",
    "        self.current_index += 1\n",
    "        if self.current_index >= self.N:\n",
    "            self.current_index = 0\n",
    "            random.shuffle(self.indexes)\n",
    "        return self.images[self.indexes[self.current_index]], self.texts[self.indexes[self.current_index]]\n",
    "\n",
    "    def next_batch(self):\n",
    "        while True:\n",
    "            # width and height are backwards from typical Keras convention\n",
    "            # because width is the time dimension when it gets fed into the RNN\n",
    "            if K.image_data_format() == 'channels_first':\n",
    "                X = np.ones([self.batch_size, 1, self.img_width, self.img_height])\n",
    "            else:\n",
    "                X = np.ones([self.batch_size, self.img_width, self.img_height, 1])\n",
    "\n",
    "            y = np.ones([self.batch_size, self.max_text_len])\n",
    "            input_length = np.ones((self.batch_size, 1)) * (self.img_width // self.downsample_factor - 2)\n",
    "            label_length = np.zeros((self.batch_size, 1))\n",
    "            source_str = []\n",
    "\n",
    "            for i in range(self.batch_size):\n",
    "                img, text = self.next_sample()\n",
    "                img = img.T\n",
    "                if K.image_data_format() == 'channels_first':\n",
    "                    img = np.expand_dims(img, 0)\n",
    "                else:\n",
    "                    img = np.expand_dims(img, -1)\n",
    "                X[i] = img\n",
    "#                 pdb.set_trace()\n",
    "#                 y_numeric = text_to_labels(text, self.letters)\n",
    "                y_numeric = text_to_labels(text)\n",
    "#                 y_numeric = [self.letter2idx[l] for l in text]\n",
    "                if self.pre_pad: padded_y = ([self.pad_idx] * (self.max_text_len - len(y_numeric))) + y_numeric\n",
    "                else: padded_y = y_numeric + ([self.pad_idx] * (self.max_text_len - len(y_numeric)))\n",
    "                y[i] = padded_y\n",
    "#                 y[i] = text_to_labels(text)\n",
    "                source_str.append(text)\n",
    "                label_length[i] = len(text)\n",
    "\n",
    "            inputs = {\n",
    "                'the_input': X,\n",
    "                'the_labels': y,\n",
    "                'input_length': input_length,\n",
    "                'label_length': label_length,\n",
    "                #'source_str': source_str\n",
    "            }          \n",
    "            outputs = {'ctc': np.zeros([self.batch_size])}\n",
    "            yield (inputs, outputs)\n",
    "\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
