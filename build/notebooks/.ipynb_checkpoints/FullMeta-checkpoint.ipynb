{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from glob import glob\n",
    "import re\n",
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "from visuals import *\n",
    "\n",
    "def get_structed_data(data_path):\n",
    "    all_data = {}\n",
    "    for filename in glob(data_path):\n",
    "        tree = ET.parse(filename)\n",
    "        root = tree.getroot()\n",
    "        tmp = []\n",
    "        ids = []\n",
    "        pos_tag = []\n",
    "        writer_id = root.attrib['writer-id']\n",
    "            \n",
    "        for part in root.findall('handwritten-part'):\n",
    "            for line in part.findall('line'):\n",
    "                for word in line.findall('word'):\n",
    "                    tmp.append(word.attrib['text'].rstrip())\n",
    "                    ids.append(word.attrib['id'].rstrip())\n",
    "                    pos_tag.append(word.attrib['tag'].rstrip())\n",
    "        assert(len(tmp) == len(ids) == len(pos_tag))\n",
    "        all_data[filename.split('/')[-1].split('.')[0]] = [tmp, ids, pos_tag]\n",
    "#         print(all_data)\n",
    "    return all_data\n",
    "\n",
    "\n",
    "def create_dataframe(all_data):\n",
    "    dat = pd.DataFrame(all_data).T.reset_index()\n",
    "    dat.columns = ['filename', 'meta', 'ids', 'pos_tag']\n",
    "    dat['folder'] = dat.filename.map(lambda x: x.split('-')[0])\n",
    "    dat['meta'] = dat.meta.map(lambda x: np.array([i.replace('&quot;', '\"') for i in x]))\n",
    "    dat['document'] = dat.filename.map(lambda x: re.sub(r'[A-Za-z]', '', x.split('-')[-1]))\n",
    "    dat['document'] = dat['folder'] + '-' + dat['document']\n",
    "    return dat\n",
    "\n",
    "\n",
    "def main_meta(data_path, save=False, save_path='../../data/preprocessed'): \n",
    "    all_data = get_structed_data(data_path)\n",
    "    dat = create_dataframe(all_data)\n",
    "    # to CSV \n",
    "    if save:\n",
    "        print('saving data to {}'.format(save_path))\n",
    "        dat.to_csv(os.path.join(save_path, 'meta.csv'), index=False)\n",
    "        dat.to_json(os.path.join(save_path, 'meta.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>meta</th>\n",
       "      <th>ids</th>\n",
       "      <th>pos_tag</th>\n",
       "      <th>folder</th>\n",
       "      <th>document</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a01-000u</td>\n",
       "      <td>[A, MOVE, to, stop, Mr., Gaitskell, from, nomi...</td>\n",
       "      <td>[a01-000u-00-00, a01-000u-00-01, a01-000u-00-0...</td>\n",
       "      <td>[AT, NN, TO, VB, NPT, NP, IN, VBG, DTI, AP, NN...</td>\n",
       "      <td>a01</td>\n",
       "      <td>a01-000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a01-000x</td>\n",
       "      <td>[A, MOVE, to, stop, Mr., Gaitskell, from, nomi...</td>\n",
       "      <td>[a01-000x-00-00, a01-000x-00-01, a01-000x-00-0...</td>\n",
       "      <td>[AT, NN, TO, VB, NPT, NP, IN, VBG, DTI, AP, NN...</td>\n",
       "      <td>a01</td>\n",
       "      <td>a01-000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a01-003</td>\n",
       "      <td>[Though, they, may, gather, some, Left-wing, s...</td>\n",
       "      <td>[a01-003-00-00, a01-003-00-01, a01-003-00-02, ...</td>\n",
       "      <td>[CS, PP3AS, MD, VB, DTI, JJB, NN, ,, AT, JJ, N...</td>\n",
       "      <td>a01</td>\n",
       "      <td>a01-003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a01-003u</td>\n",
       "      <td>[Though, they, may, gather, some, Left-wing, s...</td>\n",
       "      <td>[a01-003u-00-00, a01-003u-00-01, a01-003u-00-0...</td>\n",
       "      <td>[CS, PP3AS, MD, VB, DTI, JJB, NN, ,, AT, JJ, N...</td>\n",
       "      <td>a01</td>\n",
       "      <td>a01-003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a01-003x</td>\n",
       "      <td>[Though, they, may, gather, some, Left-wing, s...</td>\n",
       "      <td>[a01-003x-00-00, a01-003x-00-01, a01-003x-00-0...</td>\n",
       "      <td>[CS, PP3AS, MD, VB, DTI, JJB, NN, ,, AT, JJ, N...</td>\n",
       "      <td>a01</td>\n",
       "      <td>a01-003</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   filename                                               meta  \\\n",
       "0  a01-000u  [A, MOVE, to, stop, Mr., Gaitskell, from, nomi...   \n",
       "1  a01-000x  [A, MOVE, to, stop, Mr., Gaitskell, from, nomi...   \n",
       "2   a01-003  [Though, they, may, gather, some, Left-wing, s...   \n",
       "3  a01-003u  [Though, they, may, gather, some, Left-wing, s...   \n",
       "4  a01-003x  [Though, they, may, gather, some, Left-wing, s...   \n",
       "\n",
       "                                                 ids  \\\n",
       "0  [a01-000u-00-00, a01-000u-00-01, a01-000u-00-0...   \n",
       "1  [a01-000x-00-00, a01-000x-00-01, a01-000x-00-0...   \n",
       "2  [a01-003-00-00, a01-003-00-01, a01-003-00-02, ...   \n",
       "3  [a01-003u-00-00, a01-003u-00-01, a01-003u-00-0...   \n",
       "4  [a01-003x-00-00, a01-003x-00-01, a01-003x-00-0...   \n",
       "\n",
       "                                             pos_tag folder document  \n",
       "0  [AT, NN, TO, VB, NPT, NP, IN, VBG, DTI, AP, NN...    a01  a01-000  \n",
       "1  [AT, NN, TO, VB, NPT, NP, IN, VBG, DTI, AP, NN...    a01  a01-000  \n",
       "2  [CS, PP3AS, MD, VB, DTI, JJB, NN, ,, AT, JJ, N...    a01  a01-003  \n",
       "3  [CS, PP3AS, MD, VB, DTI, JJB, NN, ,, AT, JJ, N...    a01  a01-003  \n",
       "4  [CS, PP3AS, MD, VB, DTI, JJB, NN, ,, AT, JJ, N...    a01  a01-003  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../../data/raw/word_level'\n",
    "meta_data_path = '../../data/preprocessed/meta.csv'\n",
    "meta_json_data_path = '../../data/preprocessed/meta.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mattevanoff/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/Users/mattevanoff/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import re\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def duplicate_row(df, col_name):\n",
    "    \"\"\"When cell contents are lists, create a row for each element in the list\"\"\"\n",
    "    series = df.apply(lambda x: pd.Series(x[col_name]),axis=1).stack().reset_index(level=1, drop=True)\n",
    "    series.name = col_name\n",
    "    return series\n",
    "    \n",
    "def create_word_level_df(df, cols=[]):\n",
    "    \"\"\"Combine multiple Series into a pandas DataFrame\"\"\"\n",
    "    meta_series = duplicate_row(df, 'meta')\n",
    "    id_series = duplicate_row(df, 'ids')\n",
    "    pos_series = duplicate_row(df, 'pos_tag')\n",
    "    df = df.drop(['meta', 'ids', 'pos_tag'], axis=1).join(pd.concat([meta_series, id_series, pos_series], axis=1))\n",
    "    return df.rename(columns={'meta': 'token', 'ids': 'image_name'}).reset_index()\n",
    "\n",
    "def absoluteFilePaths(directory):\n",
    "    \"\"\"Walk filepaths\"\"\"\n",
    "    for dirpath,_,filenames in os.walk(directory):\n",
    "        for f in filenames:\n",
    "            yield os.path.join(dirpath, f)\n",
    "\n",
    "def create_image_path(df, data_path):\n",
    "    \"\"\"Create dictionary for mapping of word to data path\"\"\"\n",
    "    all_paths = [i for i in absoluteFilePaths(data_path)]\n",
    "    all_path_endings = [i.split('/')[-1].split('.')[0] for i in all_paths]\n",
    "    all_path_dict = defaultdict(lambda: 0, dict(zip(all_path_endings, all_paths)))\n",
    "    df['image_path'] = df['image_name'].map(lambda x: all_path_dict[x])\n",
    "    return df\n",
    "\n",
    "\n",
    "def main_word_level(meta_json_data_path, image_data_path, test_size=0.2, save=False, \n",
    "                                        save_path='../../data/preprocessed'):\n",
    "    meta = pd.read_json(meta_json_data_path)\n",
    "    meta.drop(index=494, axis=1, inplace=True)\n",
    "    word_level_df = create_word_level_df(meta)\n",
    "    word_level_df = create_image_path(word_level_df, image_data_path)\n",
    "    # split to train / test \n",
    "    train, test = train_test_split(word_level_df, test_size=test_size, random_state=100)\n",
    "    if save:\n",
    "        train.to_csv(os.path.join(save_path, 'word_level_train.csv'), index=False)\n",
    "        test.to_csv(os.path.join(save_path, 'word_level_test.csv'), index=False)\n",
    "    \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    image_data_path = '../../data/raw/word_level'\n",
    "    meta_json_data_path = '../../data/preprocessed/meta.json'    \n",
    "    main_word_level(meta_json_data_path, image_data_path, save=False, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>filename</th>\n",
       "      <th>folder</th>\n",
       "      <th>document</th>\n",
       "      <th>token</th>\n",
       "      <th>image_name</th>\n",
       "      <th>pos_tag</th>\n",
       "      <th>image_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>d05-040</td>\n",
       "      <td>d05</td>\n",
       "      <td>d05-040</td>\n",
       "      <td>Or</td>\n",
       "      <td>d05-040-00-00</td>\n",
       "      <td>CC</td>\n",
       "      <td>../../data/raw/word_level/d05/d05-040/d05-040-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>d05-040</td>\n",
       "      <td>d05</td>\n",
       "      <td>d05-040</td>\n",
       "      <td>when</td>\n",
       "      <td>d05-040-00-01</td>\n",
       "      <td>WRB</td>\n",
       "      <td>../../data/raw/word_level/d05/d05-040/d05-040-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>d05-040</td>\n",
       "      <td>d05</td>\n",
       "      <td>d05-040</td>\n",
       "      <td>he</td>\n",
       "      <td>d05-040-00-02</td>\n",
       "      <td>PP3A</td>\n",
       "      <td>../../data/raw/word_level/d05/d05-040/d05-040-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>d05-040</td>\n",
       "      <td>d05</td>\n",
       "      <td>d05-040</td>\n",
       "      <td>found</td>\n",
       "      <td>d05-040-00-03</td>\n",
       "      <td>VBD</td>\n",
       "      <td>../../data/raw/word_level/d05/d05-040/d05-040-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>d05-040</td>\n",
       "      <td>d05</td>\n",
       "      <td>d05-040</td>\n",
       "      <td>cock-fighting</td>\n",
       "      <td>d05-040-00-04</td>\n",
       "      <td>NN</td>\n",
       "      <td>../../data/raw/word_level/d05/d05-040/d05-040-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index filename folder document          token     image_name pos_tag  \\\n",
       "0      0  d05-040    d05  d05-040             Or  d05-040-00-00      CC   \n",
       "1      0  d05-040    d05  d05-040           when  d05-040-00-01     WRB   \n",
       "2      0  d05-040    d05  d05-040             he  d05-040-00-02    PP3A   \n",
       "3      0  d05-040    d05  d05-040          found  d05-040-00-03     VBD   \n",
       "4      0  d05-040    d05  d05-040  cock-fighting  d05-040-00-04      NN   \n",
       "\n",
       "                                          image_path  \n",
       "0  ../../data/raw/word_level/d05/d05-040/d05-040-...  \n",
       "1  ../../data/raw/word_level/d05/d05-040/d05-040-...  \n",
       "2  ../../data/raw/word_level/d05/d05-040/d05-040-...  \n",
       "3  ../../data/raw/word_level/d05/d05-040/d05-040-...  \n",
       "4  ../../data/raw/word_level/d05/d05-040/d05-040-...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_level_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(word_level_df, test_size=0.2, random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>filename</th>\n",
       "      <th>folder</th>\n",
       "      <th>document</th>\n",
       "      <th>token</th>\n",
       "      <th>image_name</th>\n",
       "      <th>pos_tag</th>\n",
       "      <th>image_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>64105</th>\n",
       "      <td>860</td>\n",
       "      <td>e04-011</td>\n",
       "      <td>e04</td>\n",
       "      <td>e04-011</td>\n",
       "      <td>complete</td>\n",
       "      <td>e04-011-07-06</td>\n",
       "      <td>VB</td>\n",
       "      <td>../../data/raw/word_level/e04/e04-011/e04-011-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80300</th>\n",
       "      <td>1070</td>\n",
       "      <td>b06-036</td>\n",
       "      <td>b06</td>\n",
       "      <td>b06-036</td>\n",
       "      <td>must</td>\n",
       "      <td>b06-036-01-02</td>\n",
       "      <td>MD</td>\n",
       "      <td>../../data/raw/word_level/b06/b06-036/b06-036-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86781</th>\n",
       "      <td>1158</td>\n",
       "      <td>e07-101</td>\n",
       "      <td>e07</td>\n",
       "      <td>e07-101</td>\n",
       "      <td>is</td>\n",
       "      <td>e07-101-00-06</td>\n",
       "      <td>BEZ</td>\n",
       "      <td>../../data/raw/word_level/e07/e07-101/e07-101-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89113</th>\n",
       "      <td>1188</td>\n",
       "      <td>h07-075</td>\n",
       "      <td>h07</td>\n",
       "      <td>h07-075</td>\n",
       "      <td>for</td>\n",
       "      <td>h07-075-06-01</td>\n",
       "      <td>IN</td>\n",
       "      <td>../../data/raw/word_level/h07/h07-075/h07-075-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78107</th>\n",
       "      <td>1041</td>\n",
       "      <td>g02-073</td>\n",
       "      <td>g02</td>\n",
       "      <td>g02-073</td>\n",
       "      <td>produce</td>\n",
       "      <td>g02-073-03-04</td>\n",
       "      <td>VB</td>\n",
       "      <td>../../data/raw/word_level/g02/g02-073/g02-073-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       index filename folder document     token     image_name pos_tag  \\\n",
       "64105    860  e04-011    e04  e04-011  complete  e04-011-07-06      VB   \n",
       "80300   1070  b06-036    b06  b06-036      must  b06-036-01-02      MD   \n",
       "86781   1158  e07-101    e07  e07-101        is  e07-101-00-06     BEZ   \n",
       "89113   1188  h07-075    h07  h07-075       for  h07-075-06-01      IN   \n",
       "78107   1041  g02-073    g02  g02-073   produce  g02-073-03-04      VB   \n",
       "\n",
       "                                              image_path  \n",
       "64105  ../../data/raw/word_level/e04/e04-011/e04-011-...  \n",
       "80300  ../../data/raw/word_level/b06/b06-036/b06-036-...  \n",
       "86781  ../../data/raw/word_level/e07/e07-101/e07-101-...  \n",
       "89113  ../../data/raw/word_level/h07/h07-075/h07-075-...  \n",
       "78107  ../../data/raw/word_level/g02/g02-073/g02-073-...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example.txt          meta_json.csv        word_level_test.csv\r\n",
      "meta.csv             meta_json.json       word_level_train.csv\r\n",
      "meta.json            word_level_meta.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls ../../data/preprocessed/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('../../data/preprocessed/word_level_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>filename</th>\n",
       "      <th>folder</th>\n",
       "      <th>document</th>\n",
       "      <th>token</th>\n",
       "      <th>image_name</th>\n",
       "      <th>pos_tag</th>\n",
       "      <th>image_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1111</td>\n",
       "      <td>b01-027</td>\n",
       "      <td>b01</td>\n",
       "      <td>b01-027</td>\n",
       "      <td>of</td>\n",
       "      <td>b01-027-00-05</td>\n",
       "      <td>INO</td>\n",
       "      <td>../../data/raw/word_level/b01/b01-027/b01-027-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>68</td>\n",
       "      <td>p03-185</td>\n",
       "      <td>p03</td>\n",
       "      <td>p03-185</td>\n",
       "      <td>.</td>\n",
       "      <td>p03-185-08-06</td>\n",
       "      <td>.</td>\n",
       "      <td>../../data/raw/word_level/p03/p03-185/p03-185-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1043</td>\n",
       "      <td>h01-024</td>\n",
       "      <td>h01</td>\n",
       "      <td>h01-024</td>\n",
       "      <td>as</td>\n",
       "      <td>h01-024-02-01</td>\n",
       "      <td>IN</td>\n",
       "      <td>../../data/raw/word_level/h01/h01-024/h01-024-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1162</td>\n",
       "      <td>h07-060b</td>\n",
       "      <td>h07</td>\n",
       "      <td>h07-060</td>\n",
       "      <td>the</td>\n",
       "      <td>h07-060b-03-03</td>\n",
       "      <td>ATI</td>\n",
       "      <td>../../data/raw/word_level/h07/h07-060b/h07-060...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1168</td>\n",
       "      <td>g06-050d</td>\n",
       "      <td>g06</td>\n",
       "      <td>g06-050</td>\n",
       "      <td>strong</td>\n",
       "      <td>g06-050d-04-02</td>\n",
       "      <td>JJ</td>\n",
       "      <td>../../data/raw/word_level/g06/g06-050d/g06-050...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  filename folder document   token      image_name pos_tag  \\\n",
       "0   1111   b01-027    b01  b01-027      of   b01-027-00-05     INO   \n",
       "1     68   p03-185    p03  p03-185       .   p03-185-08-06       .   \n",
       "2   1043   h01-024    h01  h01-024      as   h01-024-02-01      IN   \n",
       "3   1162  h07-060b    h07  h07-060     the  h07-060b-03-03     ATI   \n",
       "4   1168  g06-050d    g06  g06-050  strong  g06-050d-04-02      JJ   \n",
       "\n",
       "                                          image_path  \n",
       "0  ../../data/raw/word_level/b01/b01-027/b01-027-...  \n",
       "1  ../../data/raw/word_level/p03/p03-185/p03-185-...  \n",
       "2  ../../data/raw/word_level/h01/h01-024/h01-024-...  \n",
       "3  ../../data/raw/word_level/h07/h07-060b/h07-060...  \n",
       "4  ../../data/raw/word_level/g06/g06-050d/g06-050...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['of', '.', 'as', 'the', 'strong', \"Kennedy's\", 'couple', 'that', 'whole', 'commented']\n",
      "processed 0 lines\n",
      "processed 5000 lines\n",
      "processed 10000 lines\n",
      "processed 15000 lines\n",
      "processed 20000 lines\n",
      "number of skipped lines 3189\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import codecs\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from collections import Counter, defaultdict\n",
    "import re\n",
    "import dill as pickle\n",
    "import random\n",
    "import os\n",
    "\n",
    "def create_full_path(wiki_path, dataset):\n",
    "    \"\"\"Joins to paths together for wiki dataset\"\"\"\n",
    "    return os.path.join(wiki_path, 'wiki.{}.tokens'.format(dataset))\n",
    "\n",
    "def create_data(wiki_path, dataset, available_image_letters, save=False, target_sep=['<<', '>>'], \n",
    "                threshold_length=3, iteration_threshold=30, out_path='../../data/processed/'):\n",
    "    \"\"\"\n",
    "    Loads and searchs for useable target word (one we have an image for) and saves wiki text dataset\n",
    "    \n",
    "    Args:\n",
    "    \n",
    "    \n",
    "    Returns:\n",
    "    \n",
    "    \"\"\"\n",
    "    dataset_name = wiki_path.split('/')[-1]\n",
    "    data_path = create_full_path(wikitext2_path, 'train')\n",
    "    \n",
    "    random.seed(100)\n",
    "    X = []\n",
    "    y = []\n",
    "    raw = []\n",
    "    bad_words = ['=', '戦', '場', 'の', 'ヴ', 'ァ', 'ル', 'キ', 'ュ', 'リ', 'ア', '戦場のヴァルキュリア3']\n",
    "    N = 0\n",
    "    bad_lines = 0\n",
    "    counter = Counter()\n",
    "    \n",
    "    with codecs.open(data_path, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line.rstrip():\n",
    "                if not any(bad_word in line for bad_word in bad_words):\n",
    "                    for sent in sent_tokenize(line):\n",
    "                        split_sent = sent.split()\n",
    "                        found = True\n",
    "                        iteration = 0\n",
    "                        while (found == True):\n",
    "                            iteration += 1\n",
    "                            random_int = random.randint(0, len(split_sent)-1)\n",
    "                            target = split_sent[random_int]\n",
    "                            if (len(target) >= threshold_length) and (target in available_image_letters):\n",
    "#                                 X.append(split_sent[:random_int] + ['<TARGET>'] + split_sent[random_int+1:])\n",
    "                                X.append(split_sent[:random_int] + [''.join([target_sep[0]] + [target] + [target_sep[1]])] \n",
    "                                             + split_sent[random_int+1:])\n",
    "                                y.append((target, random_int))\n",
    "                                raw.append(split_sent)\n",
    "                                found = False\n",
    "                                # get word counts \n",
    "                                for token in split_sent:\n",
    "                                    counter[token] += 1\n",
    "\n",
    "                            if (iteration >= iteration_threshold):\n",
    "                                found = False\n",
    "                                bad_lines += 1\n",
    "                if N % 5000 == 0:\n",
    "                    print('processed {} lines'.format(N))\n",
    "#                 if N >= 100: break\n",
    "                N += 1\n",
    "        print('number of skipped lines', bad_lines)\n",
    "    if save:\n",
    "        print('Save Path', out_path + '----' + dataset_name + '-' + dataset)\n",
    "        np.save(out_path + 'X_' + dataset_name + '-' + dataset, X)\n",
    "        np.save(out_path + 'y_' + dataset_name + '-' + dataset, y)\n",
    "        np.save(out_path + 'raw' + dataset_name + '-' + dataset, raw)\n",
    "\n",
    "\n",
    "    return X, y, raw, counter\n",
    "\n",
    "\n",
    "def main_wiki(wikitext_path, dataset, save=False):\n",
    "    print('Reading word level meta from {}'.format(dataset))\n",
    "    word_level_meta_path = '../../data/preprocessed/word_level_{}.csv'.format(dataset)\n",
    "    # TODO if dataset = 'test' concat test and validation sets. \n",
    "    word_level_df = pd.read_csv(word_level_meta_path)\n",
    "    available_image_letters = word_level_df.token.values.tolist()\n",
    "    print('first 10 available image letters: ', available_image_letters[:10])\n",
    "    print('Building Dataset')\n",
    "    X, y, raw, counter = create_data(wikitext_path, dataset, available_image_letters, save=save)\n",
    "    return X, y, raw, counter\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    wikitext2_path = '../../data/raw/language_model/wikitext-2'\n",
    "    wikitext103_path = '../../data/raw/language_model/wikitext-103'\n",
    "    word_level_meta_path_all = '../../data/preprocessed/word_level_meta.csv'\n",
    "    word_level_meta_path_train = '../../data/preprocessed/word_level_train.csv'\n",
    "    word_level_meta_path_test = '../../data/preprocessed/word_level_test.csv'\n",
    "    \n",
    "    X, y, raw, counter = main_wiki(wikitext2_path, dataset='test', save=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77567"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77567"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(zip(X, y))[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'word_level_meta_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-f2e970e2fa27>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mword_level_meta_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'word_level_meta_path' is not defined"
     ]
    }
   ],
   "source": [
    "word_level_meta_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
